{
  "contact_info": {
    "first_name": "Ash",
    "last_name": "Manda",
    "phone": "(919) 672-2226",
    "email": "srmanda.cs@gmail.com",
    "location": "Chapel Hill, NC  • Willing to Relocate  •  Authorized to work in U.S.  •  Available full-time from May 11th, 2026",
    "linkedin_url": "https://linkedin.com/in/srmanda-cs",
    "github_url": "https://github.com/srmanda-cs",
    "portfolio_url": "https://srmanda.com"
  },
  "education": [
    {
      "institution": "The University of North Carolina at Chapel Hill",
      "degree": "Master of Science in Computer Science",
      "start_date": "2024-08",
      "graduation_date": "2026-05"
    },
    {
      "institution": "Amrita University",
      "degree": "Bachelor of Technology in Computer Science",
      "start_date": "2020-10",
      "graduation_date": "2024-06"
    }
  ],
  "work_experience": [
    {
      "job_title": "Cloud Infrastructure Engineer",
      "company": "UNC RDMC",
      "location": "Chapel Hill, NC",
      "start_date": "2026-01",
      "end_date": "Present",
      "narrative_context": {
        "technical_story": "Maintain and modernize production cloud infrastructure for UNC Research Data Management Core supporting Dataverse open-source repository platform serving UNC researchers. This role maintains UNC's single Dataverse deployment serving UNC researchers. Dataverse is open-source software trusted by 143+ institutions globally. Architect infrastructure migration from FDA-Dataverse.rdmc.unc.edu to HPO-Dataverse.rdmc.unc.edu using Terraform infrastructure-as-code across 3-4 month timeline, provisioning AWS resources: EC2 compute instances, RDS PostgreSQL databases, S3 object storage, Elastic Load Balancer for traffic distribution, SSL/TLS certificate management, VPC networking, IAM security policies. Migration planning addresses 99.999% uptime SLA (five 9's = 5.26 minutes maximum annual downtime) and 100% data integrity requirements for irreplaceable UNC research datasets supporting NIH-funded studies and FDA-related research. Design zero-downtime deployment strategy with database replication, phased cutover, rollback procedures ensuring continuous service for UNC research community. Modernize automated front-end testing infrastructure by migrating legacy Selenium test suite to Playwright framework, rewriting 24 comprehensive test cases covering Dataverse UI workflows, data submission, metadata validation, and search functionality across multiple production instances. Playwright migration improves test execution speed, reliability, and maintainability through modern async/await patterns and cross-browser compatibility. Build REDCap automated testing framework from scratch, designing test coverage for research electronic data capture platform serving clinical and translational research workflows. Architect test suites covering form validation, survey logic, branching rules, calculated fields, and data export functionality. Integrate automated testing with Azure DevOps CI/CD pipelines enabling continuous quality validation before production deployment. Collaborate across highly cross-functional 16-person team spanning 6 departments: Infrastructure (1 senior + 1 junior + 1 GRA), Development (1 lead + 1 senior + 1 junior + 1 GRA), Data Management (3), Customer Relations (3), Operations (2), Data Curation (2). Coordinate infrastructure changes with development team for application deployment, data management team for metadata standards, customer relations for researcher support, operations for monitoring, and curation team for data quality workflows.",
        "leadership_story": "Operate as infrastructure engineer within 3-person Infra team supporting production research infrastructure with 99.999% uptime SLA serving UNC research community. Take ownership of infrastructure migration project spanning 3-4 months requiring coordination across 6 cross-functional teams (16 total personnel). Drive testing modernization initiative migrating 24 legacy Selenium tests to Playwright while maintaining continuous production coverage. Lead greenfield REDCap testing framework design from requirements gathering through implementation. Balance graduate coursework with production infrastructure responsibilities requiring readiness for critical system supporting UNC investigators. Collaborate with senior and junior infrastructure engineers on architectural decisions, deployment strategies, and incident response. Interface with development team on application deployment requirements, data management team on repository standards, customer relations team on researcher pain points, operations team on monitoring and alerting, and data curation team on workflow automation. Contribute to infrastructure-as-code standards and deployment best practices enabling team scalability. Navigate production change management processes ensuring minimal risk to research data integrity and system availability.",
        "impact_story": "Support mission-critical research infrastructure for UNC investigators with 99.999% uptime SLA (5.26 minutes maximum annual downtime) and 100% data integrity requirement for irreplaceable research datasets. Deploy UNC's Dataverse instance (open-source repository software trusted by 143+ institutions globally) on AWS infrastructure ensuring reliable data storage, sharing, and discovery for UNC research community. Infrastructure supports NIH-funded studies, FDA-related research, and scientific collaboration enabling open science, data sharing, and research reproducibility across disciplines. Infrastructure migration modernizes cloud architecture improving disaster recovery capabilities, scalability for growing researcher demand, and operational efficiency through infrastructure-as-code automation. Automated testing prevents regression bugs in production deployments protecting data integrity for UNC researchers depending on platform availability for dataset uploads, metadata submission, data discovery, and reproducibility verification. REDCap testing framework ensures reliability for clinical and translational research data capture supporting human subjects research, clinical trials, and longitudinal studies. Production infrastructure uptime directly impacts UNC research velocity - system downtime halts active research workflows. Cross-functional collaboration ensures infrastructure decisions align with researcher needs, data management standards, customer support commitments, operational monitoring, and curation workflows. Terraform infrastructure-as-code enables repeatable deployments, version-controlled infrastructure changes, and disaster recovery procedures critical for maintaining five 9's availability commitment.",
        "challenges_overcome": "Primary challenge: migrating production research infrastructure serving UNC investigators while maintaining 99.999% uptime SLA and 100% data integrity. Five 9's = only 5.26 minutes annual downtime budget - migration must be zero-downtime with instant rollback capability. Solution: phased migration strategy with parallel infrastructure, database replication, DNS cutover planning, comprehensive testing in staging environment mirroring production, rollback procedures tested before execution. Challenge: coordinating infrastructure changes across 6 cross-functional teams (16 people) with competing priorities and dependencies. Development needs deployment windows, data management requires metadata validation, customer relations manages researcher communication, operations monitors system health, data curation maintains workflow continuity. Solution: structured change management process, cross-team communication protocols, staged rollouts with team-specific validation checkpoints. Challenge: modernizing 24 legacy Selenium tests to Playwright without regression in production test coverage. Legacy tests brittle, slow, difficult to maintain but currently protecting production deployments. Solution: incremental migration strategy - rewrite and validate tests in parallel with existing suite, cutover once Playwright tests achieve parity coverage, maintain legacy tests during transition period. Challenge: building REDCap testing framework from scratch without existing test infrastructure or documentation. Solution: research REDCap API documentation, analyze common researcher workflows through customer relations team feedback, design test coverage based on high-impact use cases, implement modular test architecture enabling future expansion. Challenge: ensuring data integrity for irreplaceable UNC research datasets during infrastructure changes. Research data cannot be regenerated if lost - 100% integrity non-negotiable. Solution: multiple backup strategies, database transaction validation, integrity checksums, pre-flight validation in staging, post-migration verification procedures.",
        "technologies_deep_dive": "Terraform infrastructure-as-code for declarative AWS resource provisioning and state management enabling repeatable deployments and disaster recovery. AWS cloud architecture: EC2 compute instances for application hosting, RDS PostgreSQL for relational database with automated backups and read replicas, S3 object storage for research datasets and application assets, Elastic Load Balancer for traffic distribution and high availability, SSL/TLS certificate management for encrypted communications, VPC networking for security isolation, IAM roles and policies for least-privilege access control. Dataverse open-source repository platform (Java-based architecture) deployed on Payara application server with Solr indexing engine for dataset search and discovery, trusted by 143 universities worldwide. REDCap research electronic data capture platform (PHP-based) for clinical and translational research with MySQL/PostgreSQL backend. Playwright modern testing framework using TypeScript/JavaScript for automated front-end testing with cross-browser support (Chrome, Firefox, Safari), async/await patterns for reliable asynchronous operations, configurable test environments for multiple Dataverse instances. Legacy Selenium framework migration with WebDriver automation. PostgreSQL relational database administration including replication, backup/restore, performance tuning for production workloads. Linux RHEL system administration for production server management including security patching, monitoring, log analysis. Java application server management (Payara) including deployment, configuration, performance optimization. Solr search engine configuration and indexing optimization. R language ecosystem for data analysis workflows. Kubernetes container orchestration for microservices deployment patterns. Docker containerization for application packaging and environment consistency. Azure DevOps CI/CD pipelines for automated testing integration and deployment automation. Git version control with GitHub and GitLab repository management across distributed development teams. Infrastructure monitoring and alerting supporting 99.999% uptime SLA. Database migration strategies ensuring zero-downtime and 100% data integrity. Research data compliance frameworks including NIH data sharing policy, FAIR principles (Findable, Accessible, Interoperable, Reusable), and institutional data governance. Cross-functional team collaboration spanning infrastructure, development, data management, customer relations, operations, and data curation. Production change management and incident response procedures."
      },
      "key_metrics": {
        "institution_served": "UNC",
        "uptime_sla": "99.999%",
        "annual_downtime_budget": "5.26 minutes",
        "data_integrity_requirement": "100%",
        "dataverse_software_global_adoption": "143 universities",
        "migration_timeline": "3-4 months",
        "test_cases_to_migrate": 24,
        "testing_frameworks_to_build": 1,
        "cross_functional_teams": 6,
        "total_team_size": 16,
        "infrastructure_team_size": 3,
        "aws_services": 7,
        "dataverse_instances": 2,
        "platforms_managed": 2,
        "linux_distribution": "RHEL",
        "container_technologies": 2,
        "version_control_platforms": 3,
        "deployment_environment": "production"
      },
      "role_clarity": "contributed/responsibilities. Every bullet point describes a responsibility I own or share with my team. It all needs to be in the present tense."
    },
    {
      "job_title": "Software Engineer",
      "company": "UNC Neuroscience",
      "location": "Chapel Hill, NC",
      "start_date": "2025-08",
      "end_date": "2025-12",
      "narrative_context": {
        "technical_story": "Engineered production-grade CLI tool (Pollinator v2) processing daily attendance reconciliation for neuroscience department operations serving 240 users across 6 team members. Built Python data processing platform using Pandas for DataFrame operations, NumPy for statistical analysis, and Sentence-Transformers NLP for semantic name matching. Core challenge: reconciling 240 user identities from third-party PollEv exports (inconsistent formatting, nicknames, typos) to organizational roster with daily execution requirement. Designed 4-tier matching strategy cascading through increasingly sophisticated algorithms: (1) exact string match for clean data, (2) cosine similarity scoring with 85% confidence threshold on Sentence-Transformers embeddings (all-MiniLM-L6-v2), (3) subset matching handling middle names and name variations, (4) compound first+last name verification. Implemented 3 fallback algorithms for edge cases: fuzzy string matching via Levenshtein distance, subset name analysis detecting partial matches, compound name verification combining first and last name presence. Engineered atomic transaction logic ensuring enterprise data integrity - system only modifies empty fields, never overwrites existing locked values (manually-validated records), preserving complete audit trail. CLI tool performs batch CSV processing: accepts organizational roster gradebook and multiple third-party PollEv spreadsheet exports as input, normalizes name formats handling 'Last, First' transformations and whitespace collapsing, merges data sources, performs 4-tier entity resolution cascade, updates gradebook with presence markers, generates automatic backups with version control integration. Platform executed daily saving 4 combined work hours across primary and secondary teams (60 minutes per person per session × 4 primary members = 4 hours). Normalized name formats pipeline: regex-based whitespace collapsing, special character sanitization, 'Last, First' to 'First Last' transformation, lowercase normalization for robust matching keys. Maintained Git version control for code management and rollback capability with automated backup generation for disaster recovery.",
        "leadership_story": "Guided 3 junior team members on operations and logistics while introducing modern Computer Science and Software Engineering principles into department-scale environment with legacy manual spreadsheet processes. Led cross-functional 11-person team coordinating attendance tracking, data validation, and reporting workflows. Architected CLI tool deployed daily in production serving departmental operations. When Pollinator v2 demonstrated operational success eliminating manual data entry, second team adopted the system for their attendance workflows, expanding impact to 6 total team members. Received $5,000 departmental award recognizing exceptional technical contributions to operational efficiency. Delivered knowledge transfer documentation and training enabling second team adoption without additional development cycles. Designed system architecture prioritizing maintainability for non-technical operators through clear CLI interface and automated error handling.",
        "impact_story": "Eliminated 4 hours combined daily manual data entry workload across 6 team members (primary + secondary teams) processing 240 entities with 6 attendance data points per session. Manual process required 10 seconds per update (240 entities × 6 data points ÷ 4 people = 360 updates per person = 60 minutes per session). Pollinator v2 automated entire reconciliation workflow executing daily via single command-line invocation. Daily deployment freed team members from repetitive spreadsheet population enabling focus on higher-value educational support activities. 4-tier matching cascade with 85% confidence threshold achieved robust entity resolution handling real-world data quality issues: nickname variations (Mike vs Michael), formatting inconsistencies (Last, First vs First Last), typo corrections, middle name handling. When adopted by second team, reduced workload for 2 additional team members processing similar attendance volumes demonstrating tool scalability and reusability. $5,000 award validated measurable operational impact and technical excellence. Department-scale automation transformed manual spreadsheet-driven workflow into production CLI tool with enterprise data integrity patterns.",
        "challenges_overcome": "Primary challenge: 4 hours combined daily manual workload (360 updates per person per session at 10 seconds each = 60 minutes × 4 people = 4 hours). Built working CLI prototype to demonstrate automation value replacing spreadsheet-driven manual entry. Challenge: third-party PollEv export names inconsistent with organizational roster due to nicknames (Mike vs Michael), typos, formatting variations (Last, First vs First Last), middle name handling. Solved with 4-tier matching cascade: exact match → 85% cosine similarity on Sentence-Transformers embeddings → subset matching → compound first+last verification, ensuring robust entity resolution despite data quality issues. Challenge: determining confidence threshold balancing false positives vs false negatives. Evaluated similarity scores across sample dataset, selected 85% threshold providing optimal accuracy while minimizing manual corrections. Challenge: data integrity in production environment with daily execution. Solved with atomic transaction logic - system reads existing locked values (manually-validated attendance markers), preserves them permanently, only modifies empty cells with automated matching results. Never overwrites human-validated data ensuring audit trail integrity. Challenge: enabling second team adoption without custom development. Solved with generalized CLI architecture accepting arbitrary gradebook CSV formats, configurable column mapping, automated file discovery matching PollEv exports to gradebook columns via normalized base names. Maintained Git version control and automated backup generation following enterprise disaster recovery practices.",
        "technologies_deep_dive": "Python for production CLI tool development. Pandas for DataFrame operations, CSV data manipulation, and gradebook file I/O. NumPy for statistical analysis supporting matching algorithms. Sentence-Transformers (all-MiniLM-L6-v2 model) for semantic similarity using transformer embeddings with cosine similarity scoring at 85% confidence threshold. Regular expressions for name normalization pipeline: whitespace collapsing (\\s+ → single space), special character sanitization, pattern matching for 'Last, First' format detection. 4-tier entity resolution algorithm cascading through matching strategies: (1) exact string match for clean data, (2) cosine similarity with 85% threshold on embedding vectors, (3) subset matching detecting if one name is subset of another (handles middle names), (4) compound first+last name verification checking presence of both components in candidate string. Levenshtein distance for fuzzy string matching in fallback algorithms. Atomic transaction design pattern: read locked field status, preserve existing 1 values (manually-validated), only write to empty/zero cells, maintaining complete data lineage. CLI architecture with argparse for command-line interface, Path for cross-platform file handling, batch CSV processing with automatic backup generation (_updated.csv output files). Git version control for code management, rollback capability, and change tracking. Disaster recovery procedures with automated backup maintenance before gradebook modification. Production deployment pattern: daily execution via single command-line invocation processing multiple PollEv exports against gradebook, automated file discovery matching export base names to gradebook columns via normalized keys, configurable similarity thresholds and matching strategies."
      },
      "key_metrics": {
        "users_processed": 240,
        "team_members_impacted_primary": 4,
        "team_members_impacted_secondary": 2,
        "polls_per_session": 6,
        "updates_per_person_per_session": 360,
        "manual_time_per_person_per_session": "60 minutes",
        "teams_adopted": 2,
        "award_amount": "$5,000",
        "automation_saves_daily": "4 hours combined",
        "matching_threshold": "85%",
        "matching_strategies": 4,
        "fallback_algorithms": 3,
        "model_used": "all-MiniLM-L6-v2",
        "deployment_frequency": "daily"
      },
      "role_clarity": "contributed"
    },
    {
      "job_title": "Software Development Engineer",
      "company": "Tally",
      "location": "Bengaluru, KA",
      "start_date": "2024-01",
      "end_date": "2024-05",
      "narrative_context": {
        "technical_story": "Developed Performance Management System proof-of-concept as Microsoft Teams application using TypeScript, ReactJS, Fluent UI, and Django backend. Core challenge: building cross-platform interface that rendered consistently across Teams desktop and mobile while maintaining workflow similarity to legacy system for user adoption. Teams platform imposed significant overhead - Fluent UI components behaved differently on mobile vs desktop, requiring responsive design patterns and multi-page interface architecture. Designed component hierarchy balancing legacy system familiarity with modern UX principles. Built Device Administration Portal using Next.js and Azure AD Graph API for centralized device tracking across employee and IT admin personas. Implemented role-based access control enabling employees to manage their devices without requiring Azure admin privileges - application executes privileged operations via service principal pattern. Portal surfaced active/inactive device status for employees, HR, and IT administrators.",
        "leadership_story": "Led development as primary engineer on both PoC projects during internship evaluation period. Collaborated with 3-person team on PMS PoC, driving architectural decisions and technical implementation. Made key technology decisions: selected Fluent UI and Django over legacy PHP stack to align with organization's migration strategy toward Teams-based applications. Balanced competing constraints: maintaining legacy workflow familiarity for user adoption while demonstrating modern architecture capabilities.",
        "impact_story": "Delivered cross-platform Teams application demonstrating feasibility of migrating legacy systems to modern stack. Device portal eliminated need for granting broad Azure admin access to all employees by implementing service principal delegation pattern. Enabled self-service device management reducing IT admin overhead while maintaining security controls. Demonstrated technical capabilities during internship evaluation.",
        "challenges_overcome": "Primary challenge: Fluent UI cross-platform inconsistencies between Teams desktop and mobile. Legacy PMS contained extensive field sets impossible to display on mobile without pagination. Solved by designing multi-page interface with progressive disclosure while preserving core workflow patterns from original system. Challenge: user adoption risk. Balanced modern redesign against workflow familiarity - too different would drive resistance, too similar wouldn't demonstrate value. Solution: maintained process flow, modernized presentation layer. Challenge: device portal security model. Couldn't grant Azure admin to all employees, but needed device management capabilities. Implemented service principal pattern - application executes privileged operations on behalf of authenticated users with appropriate RBAC.",
        "technologies_deep_dive": "TypeScript for type-safe development. ReactJS for component-based UI architecture. Fluent UI for Microsoft design system with Teams platform integration. Django for REST API backend. Microsoft Teams platform with cross-platform rendering challenges requiring responsive design patterns. Next.js for full-stack Device Portal with server-side rendering. Azure AD Graph API for identity and device data. Role-based access control (RBAC) implementation. Service principal pattern for delegated privilege execution. Multi-page interface architecture handling mobile constraints. Progressive disclosure UI patterns for complex data on limited screen real estate."
      },
      "key_metrics": {
        "team_size": 5,
        "platforms_supported": 3,
        "projects_delivered": 2
      },
      "role_clarity": "contributed"
    },
    {
      "job_title": "Cloud Backend Engineer",
      "company": "Vivriti Capital",
      "location": "Chennai, TN",
      "start_date": "2023-07",
      "end_date": "2023-08",
      "narrative_context": {
        "technical_story": "Architected event-driven document intelligence pipeline for financial services infrastructure processing Credit Information Reports (CIRs) - company credit histories supporting lending decisions for Vivriti Capital's debt financing platform. Built production serverless system using AWS S3, Lambda, Spring Boot, and OpenSearch. System architecture: S3 bucket receives multi-page PDF CIR documents, triggers notification to queue, queue polled at 10-minute intervals, Spring Boot orchestration layer on EC2 processes queue, creates OpenSearch indices, dispatches 4 parallel Lambda functions executing 3-minute serverless processing per document with 4GB memory allocation for section-specific parsing. Designed parallel processing architecture where each Lambda handles one CIR section using PDFPlumber with custom parsing logic targeting credit scores, total debt calculations, payment history, and company financial metrics. Key architectural decision: section-specific processing rather than uniform parsing. Analysis revealed each CIR section maintained consistency across documents but differed significantly between sections (structured tables vs unstructured text vs numeric data), enabling targeted extraction strategies maximizing accuracy. Structured extracted financial data into OpenSearch JSON documents enabling full-text search capabilities for credit analysts querying company creditworthiness, debt obligations, and risk indicators. Built ReactJS verification portal serving 30+ credit analysts for quality assurance workflow - analysts review extracted text against source CIR documents, mark accuracy, and submit corrections. Verification portal provided dual value: quality validation achieving 95% extraction accuracy with 5% requiring manual analyst corrections, and labeled dataset generation for future ML model training. Coordinated 4-person intern team across backend Lambda development, Spring Boot orchestration, OpenSearch indexing, and ReactJS verification portal frontend. Reduced months-long manual CIR analysis workflow to sub-1-day automated extraction - analysts freed from page-by-page spreadsheet population for higher-value credit risk assessment and lending decision support.",
        "leadership_story": "Led proof-of-concept development addressing previously unsolved document extraction challenge for financial services lending operations. Collaborated with ML and AI teams to evaluate technical approaches for CIR intelligence extraction. Proposed event-driven serverless architecture and parallel Lambda processing design optimizing cost and performance. Coordinated 4-person intern team on implementation across multiple technology domains: Python Lambda functions, Spring Boot orchestration, OpenSearch enterprise search, ReactJS verification interface. Delivered functional prototype within 2-month timeframe processing 200 pilot CIR documents demonstrating feasibility for production deployment. Conducted knowledge transfer to engineering team for production implementation scaling to full document pipeline. Architected verification workflow serving 30+ analysts enabling quality validation while accelerating delivery timeline. Balanced competing priorities: extraction accuracy for lending decisions, delivery speed for PoC validation, system architecture for future scalability, and labeled dataset generation for ML enhancement roadmap.",
        "impact_story": "Automated financial document analysis workflow for lending operations reducing CIR processing time from months of manual analyst effort to under one day automated extraction. Previous workflow required significant manual effort - 30+ credit analysts processing multi-page CIR reports page-by-page, populating spreadsheets with credit scores, total debt, payment history, generating risk metrics, aggregating financial data for lending decision-making. Proof-of-concept demonstrated enterprise-scale feasibility: processed approximately 200 sample CIR documents across 4 sections (credit scores, debt obligations, payment history, company financials) with automated extraction achieving 95% accuracy and searchable OpenSearch repository. 3-minute Lambda execution per document with 4GB memory allocation demonstrated cost-effective serverless processing at scale compared to months-long manual workflows. ReactJS verification portal serving 30+ analysts enabled quality validation through human-in-the-loop review workflow while building labeled training dataset for future ML enhancements. OpenSearch query capabilities enabled credit analysts to search company creditworthiness, aggregate total debt across portfolios, analyze payment history patterns, and generate risk assessment reports supporting lending decisions. System freed analysts from repetitive spreadsheet population enabling focus on higher-value credit risk analysis, loan structuring, and strategic lending recommendations. Proof-of-concept validated technical approach for production scaling across Vivriti Capital's debt financing pipeline serving enterprise lending operations.",
        "challenges_overcome": "Primary challenge: previous CIR extraction attempts failed using uniform processing approach across all document sections. Analysis identified root cause - CIR sections differed structurally (tables, paragraphs, numeric lists) despite document-to-document consistency within sections. Solution: parallel serverless architecture with 4 section-specific Lambda functions, each implementing targeted PDFPlumber parsing logic optimized for section structure (table extraction vs text parsing vs numeric pattern matching). Challenge: PDF text selection disabled by credit bureau document provider preventing simple copy-paste extraction. Evaluated OCR approaches but determined quality insufficient for financial data accuracy requirements. Solution: PDFPlumber for programmatic PDF text extraction with custom parsing rules per CIR section ensuring credit score precision and debt calculation accuracy. Challenge: balancing 3-minute Lambda execution time with 4GB memory allocation for cost-effective serverless processing. Solution: optimized PDFPlumber parsing logic, section-specific processing reducing per-function memory footprint, parallel execution amortizing total processing time across 4 concurrent Lambdas. Challenge: ensuring 95% extraction accuracy for financial lending decisions while accelerating 2-month delivery timeline. Solution: ReactJS verification portal providing dual value - 30+ analyst quality assurance workflow validating extraction accuracy through human review, and labeled dataset generation with analyst corrections building training data for future ML model refinement. Challenge: 5% manual correction rate requiring analyst intervention. Solution: verification interface enabling rapid correction submission, error pattern analysis identifying systematic extraction failures for targeted algorithm improvements, acceptable accuracy threshold for PoC validation demonstrating production viability.",
        "technologies_deep_dive": "AWS S3 for Credit Information Report document storage and event-driven triggers. AWS Lambda for serverless parallel processing - 4 Python functions executing concurrently for section-specific CIR extraction with 3-minute execution time per document and 4GB memory allocation optimizing cost-performance tradeoff. PDFPlumber for programmatic PDF text extraction with custom parsing logic targeting credit scores, total debt calculations, payment history records, and company financial metrics. Spring Boot on EC2 for orchestration layer managing queue processing, OpenSearch index creation, Lambda function dispatch, and verification portal coordination. OpenSearch for structured financial document storage, enterprise search indexing, and full-text query capabilities enabling credit analysts to search company creditworthiness, aggregate debt portfolios, analyze payment patterns, and generate risk assessment reports. Queue-based architecture with 10-minute polling intervals ensuring reliable processing for financial services lending operations. ReactJS verification portal serving 30+ credit analysts with human-in-the-loop quality assurance workflow reviewing extracted text against source CIR documents, marking accuracy, submitting corrections for 5% manual correction rate, and building labeled training dataset for future ML enhancements. Event-driven design: S3 notification → queue → Spring Boot orchestration → parallel Lambda execution (3min, 4GB per function) → OpenSearch indexing (credit scores, debt, payment history) → ReactJS verification portal (30+ analysts). Parallel Lambda architecture enabled independent section processing (structured tables, unstructured text, numeric data) and future extensibility for additional CIR document types. 4-person intern team coordination across backend Python Lambda development, Spring Boot orchestration layer, OpenSearch enterprise search configuration, and ReactJS frontend verification interface."
      },
      "key_metrics": {
        "sections_automated": 4,
        "documents_processed_pilot": 200,
        "project_delivery_time": "2 months",
        "intern_team_size": 4,
        "lambda_execution_time": "3 minutes per document",
        "lambda_memory": "4GB",
        "verification_portal_users": 30,
        "extraction_accuracy": "95%",
        "manual_correction_rate": "5%",
        "document_type": "Credit Information Reports",
        "processing_time_before": "months",
        "processing_time_after": "<1 day"
      },
      "role_clarity": "contributed"
    }
  ],
  "projects": [
    {
      "project_name": "SQuire",
      "technologies": [
        "Python",
        "C++",
        "LLVM",
        "Clang Static Analyzer",
        "LLM",
        "OpenAI API",
        "Linux Kernel",
        "Git"
      ],
      "project_url": "https://github.com/srmanda-cs/SQuire",
      "narrative_context": {
        "technical_story": "Engineered end-to-end LLM-driven pipeline synthesizing Clang Static Analyzer (CSA) checkers from Linux kernel bug-fix patches. System implements three-phase architecture: (1) Commit mining with regex-based multi-label classification targeting seven bug categories (Null Pointer Dereference, Use-Before-Initialization, Memory Leaks) across kernel versions v5.10-v5.17, filtering for atomic commits (<5 LOC, <2 files), (2) Agentic synthesis pipeline using GPT-5.1 via Poe API orchestrating pattern extraction, pattern merging, plan synthesis, and C++ checker generation with LLVM 20 API compatibility, (3) Validation framework with compilation testing, smoke testing against synthetic test cases, and historical replay using differential analysis across kernel versions. Built custom SQuire Reviewer web interface for human-in-the-loop patch curation. Implemented regex-based sanitizer correcting LLM API hallucinations (std::optional vs Optional, callback signature corrections). Designed decoupled architecture isolating checker compilation from kernel build process, enabling LLVM 20 checker compilation against older v5.x kernel trees despite API incompatibilities.",
        "leadership_story": "Conceived project vision targeting 'simple fixes' representing 87% of Linux kernel maintenance overhead. Scoped hypothesis that high-volume, low-complexity bugs contain sufficient semantic signals for LLM pattern recognition without formal verification. Led complete implementation from git mining infrastructure to agentic pipeline architecture. Designed prompt engineering strategies decomposing synthesis into discrete steps (Pattern → Plan → Code) minimizing hallucination. Managed project repository structure, API integration, and model selection. Developed lightweight testing framework validating checkers before kernel deployment.",
        "impact_story": "Reduced CSA checker synthesis cost from weeks of manual C++ development to minutes of automated generation at $0.10 per checker. Achieved zero false positives in smoke testing - checker correctly identified bug patterns without flagging valid code. Validated checker encoded allocation-source tracking logic (devm_kzalloc) rather than blanket pointer rules, demonstrating precise semantic understanding. Curated high-quality dataset in <1 day through aggressive heuristic filtering. Proved LLM-generated checkers viable with minimal human refinement, closing KNighter's validation loop at fraction of original GPU/compute cost. System targets 10% reduction in kernel maintenance burden, reclaiming thousands of developer hours for complex architectural challenges.",
        "challenges_overcome": "Primary challenge: LLM API hallucinations. GPT-5.1 invented non-existent Clang AST methods and confused LLVM version signatures despite documentation access. Solution: regex-based sanitizer automatically patching common hallucinations, plus manual refinement loop proving minimal human intervention sufficient. Challenge: LLVM 20 incompatibility with older kernel versions (v5.x API mismatches). Solution: decoupled build environment isolating checker compilation from kernel build, enabling latest LLVM compilation while analyzing legacy kernel trees. Challenge: context window limitations for massive codebases. Solution: careful diff pruning and strict output format enforcement in pipeline. Challenge: distinguishing LLM prototype from production code. Solution: focused on 'simple fixes' mitigating hallucination risk while ensuring checker performance and explainability.",
        "technologies_deep_dive": "Python for orchestration layer and pipeline implementation. C++ for synthesized CSA checker code. LLVM 20 and Clang 20 on Arch Linux (Manjaro) for bleeding-edge C++20 features and CSA APIs. Clang Static Analyzer for symbolic execution and path-sensitive bug detection. GPT-5.1 via Poe API (OpenAI-compatible) as reasoning engine. Git for kernel history mining across versions v5.10-v5.17. Regex-based pattern matching for commit classification. compile_commands.json generation using intercept-build for compilation database. Custom web interface (SQuire Reviewer) for patch curation. Agentic pipeline managing context windows through diff pruning and enforcing structured C++ output format. Sanitization layer correcting API hallucinations and legacy callback signatures. Differential analysis comparing analysis reports across kernel versions identifying fixed bugs vs noise."
      },
      "key_metrics": {
        "cost_per_checker": "$0.10",
        "curation_time": "<1 day",
        "false_positives_smoke_test": 0,
        "kernel_versions_analyzed": "v5.10-v5.17",
        "bug_categories_targeted": 7,
        "commit_filter_loc": "<5 LOC",
        "commit_filter_files": "<2 files",
        "pipeline_phases": 3,
        "maintenance_overhead_target": "10%",
        "llvm_version": 20
      },
      "role_clarity": "led"
    },
    {
      "project_name": "StreamJack Vulnerability",
      "technologies": [
        "Python",
        "Cryptography",
        "Security Research",
        "AES-128",
        "Reverse Engineering",
        "JavaScript",
        "pycryptodome",
        "Browser DevTools",
        "HLS Protocol",
        "ffmpeg",
        "IDOR Exploitation"
      ],
      "project_url": "https://tinyurl.com/DRM-StreamJack",
      "narrative_context": {
        "technical_story": "Discovered and responsibly disclosed critical DRM vulnerability in educational streaming platform owned by $3.5 billion EdTech unicorn serving 200,000+ creators generating $1.5 billion+ annual creator revenue, enabling unauthorized content access through systematic exploitation of cryptographic implementation flaws. Reverse engineered custom AES-128 ECB encryption scheme by analyzing JavaScript source code in browser DevTools, identifying fundamental security architecture failures: IDOR vulnerabilities exposing sensitive identifiers (apkId, courseId, videoId), custom cryptography implementation deviating from standard libraries introducing exploitable weaknesses, and broken authentication allowing direct stream URL access without session validation. Engineered Python-based decryption tool using pycryptodome library for automated key extraction from encrypted timestamp metadata files, implementing custom byte array slicing algorithms to reconstruct private keys from obfuscated ciphertext. Integrated exploit with N_m3u8DL-RE HLS downloader for automated video stream acquisition using extracted decryption keys. Attack workflow: IDOR enumeration → stream URL extraction → encrypted key file retrieval → reverse engineering key derivation algorithm → automated decryption → content download. Documented comprehensive 12-page technical writeup with 20 detailed figures detailing attack methodology, step-by-step exploitation instructions, prerequisite tools (Python 3.12, pycryptodome 3.21, N_m3u8DL-RE 0.3.0_beta, ffmpeg), and defense recommendations. Platform vulnerability stemmed from three critical failures: directly exposing encryption parameters through client-side JavaScript, implementing non-standard cryptographic operations on standard AES primitives creating predictable patterns, and absent server-side authentication for streaming endpoints. Completed responsible disclosure following industry best practices in August 2025 - vendor acknowledged vulnerability, deployed security patches, and confirmed remediation protecting creator content ecosystem.",
        "leadership_story": "Conceived and executed independent security research project targeting DRM systems in educational technology sector. Identified high-value target: EdTech platform serving millions of users with content protection mechanisms securing premium courses for 200,000+ creators. Led end-to-end vulnerability discovery lifecycle: reconnaissance through browser-based analysis, hypothesis formulation around encryption implementation weaknesses, systematic exploit development, automation tool creation, and responsible disclosure coordination. Documented findings with publication-quality technical writeup (12 pages, 20 figures) suitable for security community education. Made strategic decision to withhold public exploit code repository to prevent script kiddie abuse while publishing detailed methodology for security researcher awareness and defender education. Balanced transparency with responsibility: detailed enough for defenders to understand attack vectors and implement proper fixes, careful enough to prevent immediate weaponization by unsophisticated attackers. Initiated vendor contact through responsible disclosure channels in August 2025, maintained professional communication throughout remediation process, verified patch effectiveness before public disclosure. Project demonstrated security research methodology: identifying real-world cryptographic vulnerabilities in commercial systems, reverse engineering heavily obfuscated implementations, building practical exploitation tools, and conducting ethical disclosure protecting creator economy generating $1.5 billion+ revenue.",
        "impact_story": "Identified critical security vulnerability undermining entire content protection infrastructure of platform serving educational content to global creator community generating $1.5 billion+ annual revenue. Attack exploited combination of three vulnerability classes (IDOR, custom cryptography, broken authentication) creating complete security bypass threatening creator monetization model. Platform stake: $3.5 billion valuation EdTech unicorn with 200,000+ creators dependent on video DRM for revenue protection. Vulnerability impact: any authenticated user could download unlimited encrypted content, private encryption keys directly extractable from client-side code, stream URLs accessible without additional authentication, complete DRM failure exposing all premium creator content. Business risk: potential unauthorized content redistribution at scale undermining creator revenue streams, competitive intelligence exposure for exclusive educational materials, reputational damage to platform trust. Responsible disclosure in August 2025 prevented large-scale exploitation: vendor confirmed vulnerability within disclosure timeline, deployed security patches addressing all three vulnerability classes, acknowledged contribution to platform security protecting $1.5 billion+ creator economy. Technical writeup provided actionable defense guidance: never expose encryption keys client-side, use standard cryptographic implementations without custom modifications, implement robust server-side authentication for streaming endpoints, adopt industry-standard DRM solutions (Widevine, FairPlay) instead of custom schemes. Educational impact: 12-page documentation serves as case study for security engineering courses on cryptographic implementation failures, IDOR vulnerability chains, and responsible disclosure practices balancing security researcher ethics with business continuity.",
        "challenges_overcome": "Primary challenge: reverse engineering heavily obfuscated JavaScript cryptography implementation. Vendor employed code obfuscation, variable name mangling, and complex control flow to hide encryption logic protecting 200,000+ creator content. Solution: systematic DevTools analysis, pattern recognition in minified code, dynamic debugging to trace execution flow, identifying critical code blocks handling key derivation protecting $1.5 billion+ revenue streams. Challenge: deciphering custom byte array slicing algorithm for key reconstruction. Standard AES-128 implementations don't require substring operations on encryption keys, but custom implementation concatenated specific byte ranges (apkId[:5] + apkId[37:], tmp1[48:64], tmp2[16:32]) creating non-standard key derivation. Solution: iterative experimentation with range combinations, cross-referencing with JavaScript source code patterns, validating against successful decryptions. Challenge: handling dynamic range modifications across platform updates. Platform developers frequently updated obfuscation patterns, changing byte ranges across deployments to protect content. Solution: documented reverse engineering methodology in 12-page writeup enabling range identification for any code version, ensuring defenders understood attack persistence across updates. Challenge: responsible disclosure logistics with $3.5 billion company. No security contact listed, no bug bounty program, large company potentially unresponsive to independent researchers. Solution: persistent communication through multiple channels, professional documentation of findings, patient coordination until acknowledgment received in August 2025. Challenge: balancing public education with preventing script kiddie exploitation of $1.5 billion+ creator economy. Solution: detailed methodology publication without releasing turnkey exploit code, requiring technical sophistication to replicate while enabling defender understanding and proper security fixes.",
        "technologies_deep_dive": "Python 3.12 for exploit automation and key extraction scripting protecting $1.5 billion+ creator content. pycryptodome 3.21.0 for AES-128 decryption operations - standard cryptographic library providing ECB mode implementation. Crypto.Cipher.AES module for instantiating cipher objects with extracted keys. N_m3u8DL-RE 0.3.0_beta for HLS protocol handling - specialized downloader supporting custom decryption key injection for encrypted video streams. ffmpeg for media processing and format conversion. Browser DevTools (Vivaldi 7.0) for JavaScript analysis, network traffic inspection, DOM manipulation, and dynamic debugging of obfuscated code protecting creator revenue. JavaScript reverse engineering for analyzing minified source code, tracing encryption algorithms, identifying vulnerability patterns in DRM implementation. HLS (HTTP Live Streaming) protocol understanding for m3u8 playlist parsing, encrypted chunk handling, AES-128-CBC stream decryption. IDOR (Insecure Direct Object Reference) exploitation techniques for parameter enumeration and access control bypass. AES-128 ECB mode cryptanalysis - understanding mode weaknesses, pattern recognition in encrypted data, key derivation vulnerabilities. Browser console manipulation for extracting client-side variables (apkId extraction via console.log). Network traffic analysis for identifying stream URLs, encryption key endpoints, authentication flow weaknesses. Byte array operations and hexadecimal encoding for binary data manipulation. Base64 encoding for key format conversion. Custom algorithm reverse engineering combining static analysis (code reading) with dynamic analysis (runtime debugging). Responsible disclosure protocols following industry best practices protecting 200,000+ creators."
      },
      "key_metrics": {
        "platform_valuation": "$3.5 billion",
        "creators_affected": "200,000+",
        "creator_revenue_at_risk": "$1.5 billion+",
        "vulnerability_classes": 3,
        "documentation_pages": 12,
        "documentation_figures": 20,
        "disclosure_date": "August 2025",
        "tools_developed": 1,
        "vendor_response": "Patched",
        "encryption_broken": "AES-128 ECB",
        "prerequisites_documented": 5,
        "attack_steps": 20,
        "python_version": "3.12.1",
        "pycryptodome_version": "3.21.0",
        "n_m3u8dl_version": "0.3.0_beta",
        "disclosure_outcome": "Vendor confirmed and remediated"
      },
      "role_clarity": "led"
    },
    {
      "project_name": "Jobbernaut Tailor",
      "technologies": [
        "Python",
        "FastAPI",
        "OpenAI API",
        "Jinja2",
        "LaTeX",
        "Pydantic",
        "YAML",
        "CLI"
      ],
      "project_url": "https://github.com/Jobbernaut/jobbernaut-tailor",
      "narrative_context": {
        "technical_story": "Engineered 13-step intelligence-driven pipeline with 3-stage intelligence gathering phase executing BEFORE content generation: (1) Job resonance analysis extracting emotional keywords, cultural values, hidden requirements, power verbs, and technical keywords, (2) Company research mapping mission, values, tech stack, and culture, (3) Storytelling arc generation creating narrative hooks and proof points. Pipeline then: (4) Validates job inputs, (5) Generates tailored resume using intelligence context, (6) Validates with Pydantic, (7) Applies humanization, (8) Renders LaTeX via Jinja2 templates, (9-13) Compiles PDFs and organizes output. Built master_resume.json as single source of truth grounding all LLM outputs. Implemented multi-layer validation: input validation → Pydantic schema validation → quality threshold validation (character counts, array sizes, meaningful content). Designed generic retry wrapper with progressive error feedback enabling self-healing across all intelligence steps. Fully open source with comprehensive technical documentation (7 guides: ARCHITECTURE, EVOLUTION, PIPELINE, MODELS, CONFIGURATION, TEMPLATES, ANTI_FRAGILE_PIPELINE). PDFs optimized to <30 KiB. Workday: uploaded resume, zero fields needed editing, perfect auto-population.",
        "leadership_story": "Conceived and single-handedly developed entire system from tracker idea to production CLI tool. Started with Lovable tracker, posted on LinkedIn, received 5 access requests. Did market research, found competitors selling spreadsheets for $5. Realized actual problem wasn't tracking but tailoring. Pivoted completely. Gave trial to 5 roommates as beta testers. They applied to 50 jobs in a day with zero fake content, perfectly tailored. Labeled Jobbernaut v1, took offline because realized market disruption potential. Iterated through v2 (production freeze with LaTeX verification), v3 (template revolution with Jinja2), and v4 (intelligence-driven architecture). Open sourced entire codebase with professional documentation enabling community adoption. Architected system to be anti-fragile: learns from mistakes, self-healing via progressive error feedback, error-correcting with quality validation.",
        "impact_story": "Reduced tailoring time from 30-40 minutes to under 2 minutes through automated intelligence gathering and template rendering. Beta testers applied to multiple positions daily while maintaining content quality and factual accuracy. Achieved strong ATS compatibility scores on Enhancv and Jobscan through LaTeX formatting control. Workday parsing handled resume uploads with minimal manual field corrections. PDFs optimized to <30 KiB ensuring fast ATS processing. Open sourced with comprehensive technical documentation (7 guides) enabling community understanding and contribution. Intelligence-driven architecture produces targeted applications through contextual analysis before content generation.",
        "challenges_overcome": "Initial: preventing LLM hallucinations. Solved by grounding with master_resume.json and multi-layer validation (input → Pydantic → quality thresholds). Challenge: ATS compatibility. Solved using LaTeX for precise formatting control and PDF size optimization. Moved from LLM-generated LaTeX to Jinja templates for reliability. Challenge: factual accuracy. Solved with Pydantic schema validation, quality threshold validation, and explicit verification prompts. Challenge: balancing automation speed with quality. Solved with 13-step pipeline where each step has single responsibility. V4 challenge: reactive content generation producing generic outputs. Solved by separating intelligence gathering from content generation - understand job context first, then generate targeted content. Challenge: ensuring meaningful content beyond structural validity. Solved with quality validation layer checking character counts, array sizes, and content substance. Challenge: code reusability across intelligence steps. Solved with generic retry wrapper handling all intelligence steps uniformly with progressive error feedback.",
        "technologies_deep_dive": "Python for pipeline orchestration. OpenAI API (Poe AI) for intelligent content generation across multiple models (Gemini, Claude, GPT-4). Pydantic for strict JSON schema validation preventing malformed outputs. Quality validation layer beyond Pydantic ensuring meaningful content (character thresholds, array size constraints, empty string detection). Jinja2 templating with custom delimiters (\\VAR{}, \\BLOCK{}) for LaTeX code generation (replaced direct LLM generation). LaTeX for precise PDF formatting and ATS compatibility. YAML for human-readable application storage. CLI built with Python argparse. pdflatex compiler for PDF generation. Git for version control. Generic retry wrapper with progressive error feedback enabling self-healing. Configurable AI models per intelligence step optimizing for speed, creativity, or accuracy. Fail-fast input validation preventing wasted API calls. Designed as anti-fragile system: learns from mistakes, self-healing, error-correcting. Each pipeline step isolated for independent testing. Comprehensive documentation system with 7 technical guides."
      },
      "key_metrics": {
        "pipeline_steps": 13,
        "intelligence_steps": 3,
        "beta_testers": 5,
        "pdf_size": "<30 KiB",
        "versions_developed": 4,
        "documentation_pages": 7,
        "tailoring_time_before": "30-40 minutes",
        "tailoring_time_after": "under 2 minutes",
        "open_source": true
      },
      "role_clarity": "led"
    },
    {
      "project_name": "MatchWise",
      "technologies": [
        "Python",
        "PuLP",
        "FastAPI",
        "Flutter",
        "Integer Linear Programming",
        "REST API"
      ],
      "project_url": "https://github.com/yashas-hm-unc/matchwise-backend/tree/main",
      "narrative_context": {
        "technical_story": "Formulated graduate student-to-professor assignment as Integer Linear Programming problem using Python's PuLP library. Designed custom objective function generating 'best fit' probability score based on mutual rankings and research interest alignment. Input: professors list with ranked student preferences, students list with ranked professor preferences, research interests from CS directory. Constraint: each student assigned to exactly one professor, professor capacity limits respected. Optimization goal: maximize total fit probability. Built FastAPI backend exposing matching algorithm as REST API. Teammates developed Flutter frontend. Near project end, realized tight coupling. Quickly pivoted to decoupled architecture with blackbox functions, enabling independent hosting.",
        "leadership_story": "Led backend development and core ILP solution design while teammates worked on Flutter frontend. Worked with client (CS department) to understand pain points in manual matching. Identified 5-day process considering 200+ factors. Recognized as optimization problem rather than just UI problem. Delivered software in sprint-based approach with each team member having distinct role. Communicated via JSON payloads. Genuine industrial-grade software engineering project requiring client requirement gathering, technology selection justification, product delivery.",
        "impact_story": "Reduced matching from 5 days manual work to 50-100ms automated response (99.9% reduction). Handles 30-40 professors and 60-100 students annually. Considers 200 different factors automatically (professor fit, availability, research interests, mutual preferences). Eliminates human bias, ensures optimal global matching rather than greedy local decisions. Clicking match button returns fully matched set within 100ms tested via Postman. While not adopted by department, proved concept viable and demonstrated optimization approach to complex assignment problem.",
        "challenges_overcome": "Challenge: formulating fuzzy matching problem as mathematical optimization. Solved by designing probability-based objective function quantifying 'fit' from qualitative preferences. Challenge: handling conflicting preferences (student wants professor A, but professor A prefers different student). Solved by ILP constraint satisfaction finding globally optimal solution. Challenge: tight coupling between frontend and backend limiting independent development. Solved by refactoring to blackbox API functions with clear JSON contracts. Challenge: ensuring fast response time for real-time matching. Solved by optimizing PuLP solver configuration and constraint formulation.",
        "technologies_deep_dive": "Python for backend logic and optimization. PuLP (Python Linear Programming) library for ILP problem formulation and solving. FastAPI for high-performance REST API with automatic OpenAPI documentation. Flutter for cross-platform mobile/web frontend. JSON for API communication contracts. Integer Linear Programming for constraint satisfaction and optimization. REST API architecture for decoupled frontend-backend communication. Postman for API testing and performance validation. Research interest matching using text similarity algorithms."
      },
      "key_metrics": {
        "manual_process_time": "5 days",
        "factors_considered": 200,
        "professors_count": "30-40",
        "students_count": "60-100",
        "response_time": "50-100ms",
        "time_reduction_percentage": "99.9%"
      },
      "role_clarity": "led"
    }
  ],
  "skills": {
    "Programming Languages": "Python, JavaScript, TypeScript, Java, C++, SQL, C#",
    "Frontend & Testing": "React, Next.js, Fluent UI, Playwright, Selenium, Jinja2, Flutter",
    "Backend & APIs": "FastAPI, Django, Spring Boot, Flask, Node.js, REST API, Microservices",
    "Cloud & Infrastructure": "AWS (Lambda, S3, EC2, RDS), Terraform, Docker, Kubernetes, Azure DevOps",
    "Databases & Search": "PostgreSQL, MySQL, OpenSearch, Redis, MongoDB, RDS",
    "Data & ML": "Pandas, NumPy, Sentence-Transformers, PyTorch, NLP, ETL Pipelines, PuLP",
    "Static Analysis": "LLVM, Clang Static Analyzer, Clang, AST Manipulation",
    "DevOps & Tools": "Git, CI/CD, Linux (RHEL), REDCap, Dataverse, LaTeX, Agile",
    "Core Competencies": "System Architecture, Algorithm Design, Security Research, Performance Optimization"
  }
}
