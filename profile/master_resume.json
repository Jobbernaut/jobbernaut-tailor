{
  "contact_info": {
    "first_name": "Snehashish Reddy",
    "last_name": "Manda",
    "phone": "9196722226",
    "email": "srmanda.cs@gmail.com",
    "location": "Chapel Hill, NC",
    "linkedin_url": "https://linkedin.com/in/srmanda-cs",
    "github_url": "https://github.com/srmanda-cs",
    "portfolio_url": "https://srmanda.com"
  },
  "education": [
    {
      "institution": "UNC Chapel Hill",
      "degree": "Master of Science in Computer Science (GPA: 4.0/4.0)",
      "start_date": "2024-08",
      "graduation_date": "2026-05"
    },
    {
      "institution": "Amrita Vishwa Vidyapeetham",
      "degree": "Bachelor of Technology in Computer Science (GPA: 8.86/10.0)",
      "start_date": "2020-10",
      "graduation_date": "2024-06"
    }
  ],
  "work_experience": [
    {
      "job_title": "Software Development Engineer (Platform)",
      "company": "UNC Chapel Hill",
      "location": "Chapel Hill, NC",
      "start_date": "2025-08",
      "end_date": "Present",
      "narrative_context": "Graduate Instructional Assistant role (written as SDE to attract recruiters, though most work was logistical). Professor Sabrina Robertson used PollEv to track participation for 240 students across 2 sections. With 6 PollEvs per class, that's 6×240 responses to process. 4 GTAs handled this, meaning 6×60=360 grade updates per person per class. At 10 seconds per update, that's 36 minutes per person per day, or 180 minutes per week per person - highly inefficient. Proposed solution: since Canvas gradebook and PollEv responses are downloadable, this is an engineering problem solvable with AI. Built Python script using NLP Sentence-Transformers, statistical name analysis, first/last name matching to match PollEv names to gradebook names with high accuracy. Script downloads gradebook from Canvas, accesses PollEv account through spreadsheet mapping which PollEv column corresponds to which question. Challenges: merging multiple spreadsheets from different class times, avoiding bot detection. Tool worked seamlessly - given spreadsheet, running Python script auto-updates grades. Instructor and TA from another course approached after seeing success. Both courses now experimenting; if successful, every instructor in university will use it. Technologies: Pandas, NumPy, Sentence Transformers, keyword matching libraries. 10 instructors in our course + 3 in other course = 13 total. Nobody cares about PollEv anymore - major victory. University-wide deployment would just need Lambda function. Ensured atomicity: gradebook only pulls PollEv view, if number exists (0/1) it can't be modified, only dashes can change. Students missing 1-2 questions: if answered >3 out of 6 (>50%), credit updated to 1. Rest of dashes populated with zeros. Designed highly specific standardized rubrics to minimize decision-making - reduced grading time from 5-6 minutes to 30 seconds per writing assignment. Implemented version control for autograding platform so one change doesn't break system, can fallback to known working version. Automated backups for gradebooks in case system makes mistake (hasn't happened yet). Previously tracked everything on single monolithic Google doc. Introduced Trello and Jira - eliminated need for many meetings, tasks posted on Trello, Jira tickets for specific assignments. Nightmare convincing team but massive help. Work reduced from 20 hours required by policy to barely 3 hours per week after mandatory 5 hours course attendance. This 8-hour reduction resulted from both autograder (3+ hours saved) and logistical improvements (8-9 hours saved). Convinced team by running pilot program with another grader, others just observed. When work got done faster with less effort, showed examples from experienced professors using these tools, demonstrated education templates. Eventually convinced. Did not lead - professor led, ULAs under me, professor above me, GTAs were colleagues. Guided and trained ULAs on research and logistics while bringing CS concepts into Neuroscience. Memory Systems II lecture: brutally difficult, professor asked to prepare 3 months in advance, massive research effort. $5,000 award given as one-time exception for overall contributions - reached out about high tuition, was doing good work, leadership appreciated it.",
      "key_metrics": {
        "students_managed": 240,
        "course_sections": 2,
        "gtas_on_team": 4,
        "grade_updates_per_person_per_class": 360,
        "manual_time_per_person_per_day": "36 minutes",
        "manual_time_per_person_per_week": "180 minutes",
        "pollev_questions_per_class": 6,
        "total_instructors_our_course": 10,
        "total_instructors_other_course": 3,
        "total_instructors_using_tool": 13,
        "courses_adopted": 2,
        "grading_time_before": "5-6 minutes per assignment",
        "grading_time_after": "30 seconds per assignment",
        "weekly_hours_required_by_policy": 20,
        "weekly_hours_after_optimization": 8,
        "time_saved_from_autograder": "3+ hours per week",
        "time_saved_from_logistics": "8-9 hours per week",
        "total_time_saved": "12 hours per week",
        "actual_work_hours_per_week": "3 hours (excluding 5 hours mandatory attendance)",
        "award_amount": "$5,000",
        "lecture_prep_time": "3 months for Memory Systems II"
      },
      "role_clarity": "contributed",
      "bullet_points": [
        "Spearheaded the creation of a Python-based autograding platform to automate participation tracking for 240 students across two course sections, addressing a highly inefficient and manual process.",
        "Engineered a robust student name-matching algorithm using Sentence-Transformers (NLP) and statistical analysis to reconcile inconsistent PollEv data with official Canvas rosters, achieving high-accuracy matches.",
        "Architected an atomic grade-update process using Pandas that prevented data corruption by only modifying empty fields, ensuring data integrity across thousands of weekly grade entries.",
        "Developed and implemented business logic to automatically award credit based on participation thresholds (e.g., >50% of polls answered), standardizing the grading policy.",
        "Championed and integrated Jira and Trello into the 10-person instructional team's workflow, replacing a monolithic document, reducing excessive meetings, and saving an estimated 8-9 hours of logistical overhead weekly.",
        "Designed and deployed standardized, decision-optimized grading rubrics for written assignments, slashing the grading time per assignment from ~6 minutes to just 30 seconds.",
        "The combined automation and process optimization initiatives reduced the total weekly workload for the instructional team from a required 20 hours to just 8 hours, a 60% reduction in time commitment.",
        "Ensured system resilience by implementing version control for the automation scripts and establishing automated backups for the Canvas gradebook, safeguarding against data loss.",
        "Successfully piloted the tool, which led to its adoption by an additional university course and positioned it for a potential university-wide deployment as a serverless AWS Lambda function."
      ]
    },
    {
      "job_title": "Software Development Engineer (Internal Systems)",
      "company": "Tally",
      "location": "Bengaluru, KA, IN",
      "start_date": "2024-01",
      "end_date": "2024-05",
      "narrative_context": "Me and two other interns under a manager at Tally. Manager trained us in React; I was already well-trained, so because I showed good communication, rapport, and technical knowledge, manager left it to me to deal with interns - he primarily communicated with me. Tally had ancient PMS system from 2000 where employees set quarterly goals, went through multiple manager reviews (skip level, scrum, etc.) for approval. If approved, had to meet goals or explain why not. Three review levels: quarterly, half-yearly, annually. Promotions solely based on review reports from this system. Problem: system written in 2000 by founder, now 2024 - 24 years later nobody knew how/why it worked, just that it worked. Employees impatient saying nobody knows how it works, hard to access, can't see what's happening, being treated unfairly. Management saw three fresh interns, said let them prove if project viable. If yes, invest in new system; if no, can tell employees why not possible. Manager was one who complained about legacy PMS, championed for new PMS, strung it this far - that's why put in charge. Reality: sword hanging over my head. Hadn't finished college, had to manage two interns, deal with manager/coworkers, pressure of delivering in 3 months because management against new PoC since current system worked fine. Most internal apps on Teams - discussed legacy Xamarin vs Flutter vs Teams. Flutter is Google thing, might disappear; Teams massive, not going away, runs React websites. Chose ReactJS + Fluent UI + NodeJS as Teams app called 'PMS' accessible from phones. Biggest issue with old PMS: lack of transparency. Performance review approved? Yes. No place for strong feedback. Management busy, takes 5 minutes for review, once approved/declined disappears from system - only records approval/rejection status. Lower level employee sees 'SLM declined your report', must start over. 30-40 employees under SLM, no time for personal meetings explaining why approved/declined. Naturally everything approved to avoid headache. System that 'just worked' didn't work at all. Made in 2000, who had this in mind? When properly declined, employees saw as personal attack and quit because everyone else approved, no reason given why. Company churning talent. Had to showcase transparent pipeline, slot in well with current system for easy migration, give place for feedback, keep UX good for everyone. Sat down, talked to everyone in performance review process, mapped architecture. PoC included goal setting: daily, weekly, monthly, quarterly, half-yearly, annual. Employee can set personal goals, higher management sets organizational goals trickling down, team goals for team, personal goals for self - purely transparent. Backend: Flask on MySQL (what old PMS used) for easy slot in/out with data. Generated leaderboards, specific reports for each management level showing how many employees marked goals done. Manager guided me on what to do; I mapped what needs doing at code/architectural level, split among us three, got development done quickly. Showed management - keeping PoC performance aside, most importantly showed system worked, was transparent, convenient for everyone. Project taken from us, new PMS budget approved at ₹7cr (~$1M at the time) due to engineering effort required for legacy compatibility + modern features, handed to talented senior engineering team. We participated in KT through documentation of architecture, stack. Pretty sure it was scrapped because PoC wasn't state of the art - main point was proving it could be done. Seeing our success (especially mine), manager happy because point proven, got promotion. Other interns left struggling for project, I immediately reassigned to smaller but big impact project. Company had ₹1.5 lakh laptops per employee, also provided iPhones, Androids, other stuff. Used onPrems to track attendance (device connected to on-site Tally network = marked present). Security incident: old employee device used to access confidential info about new version release, leaked to press. Important because release increased prices - knowing price point before launch could give competitors like SAP Labs edge to undercut. Management didn't want this again. Azure AD graph existed, onPrems storage existed, but device deletion had to be done manually by IT admins. Really bad - after initial review, manager and I concluded just in past year company lost track of ₹15 lakh (~$20,000) worth of assets because after employees left, no system to get devices back except HR email after leaving. Removed from payroll but devices remained. One man job anyway, gave me task: make new portal on HR and IT admin site integrating with current employee portal. Every employee should view current active devices, disable them. Same for HR - see on portal how many devices active at any time. Since they manually remove employee from payroll anyway, can check employee device portal and remove devices. Initial scope: I merged Azure Graph Data, MySQL device data, did complicated algorithms to alert HR and IT on devices inactive for long time. Went two steps further: if employee on leave/holiday, wouldn't need company/device access anyway. If employee marked on leave or removed from payroll, devices automatically disabled, had to be re-enabled by IT admin. Inconvenient but convinced it's acceptable for security. Didn't realize I took automations so far that most work HR did to remove employee manually, now only had to press one button 'remove employee' or 'put employee on leave', all my scripts ran behind scenes to aggregate data, confirm employee ID, remove access from device MAC address. End of internship found out my portal made HR have less work (they hired lot of people since process so manual). Heard few people's roles in IT admin and HR made redundant due to NextJS portal I designed. Doesn't speak to how good portal was, but how bad process was before. Security breaches simply can't happen anymore - employee marks self on leave on portal, immediately device access revoked, so security breaches due to old employee devices connecting to network don't exist because system precautionary. Metrics: assuming one HR and one IT admin role redundant, per year ₹15 lakh assets wasted due to this issue, employee salaries (₹6 lakh × 2 + ₹15 lakh) = ₹27 lakh (~$40,000 USD) saved per year. Pretty realistic.",
      "key_metrics": {
        "team_size": 3,
        "interns_managed": 2,
        "legacy_system_age": "24 years",
        "pms_budget_approved": "₹7 crore (~$1M USD)",
        "project_delivery_time": "3 months for PoC",
        "review_levels": 3,
        "laptop_value_per_employee": "₹1.5 lakh",
        "assets_lost_per_year": "₹15 lakh (~$20,000 USD)",
        "estimated_annual_savings": "₹27 lakh (~$40,000 USD)",
        "hr_salary_saved": "₹6 lakh × 2 = ₹12 lakh",
        "roles_made_redundant": "2 (1 HR + 1 IT admin)",
        "employees_under_slm": "30-40",
        "review_time_per_employee": "5 minutes"
      },
      "role_clarity": "led (for PoC and asset portal), enabled (for final PMS investment)",
      "bullet_points": [
        "Led a 3-engineer team to develop a Proof-of-Concept (PoC) for a modern Performance Management System (PMS) to replace a 24-year-old legacy platform that was causing high employee turnover.",
        "Architected the PMS PoC as a Microsoft Teams application using React, Fluent UI, and a Node.js/Flask backend, ensuring seamless integration with the existing MySQL database for data compatibility.",
        "The PoC's success in demonstrating a transparent, feedback-driven workflow directly convinced senior management to approve a ₹7 Cr (~$1M) budget for a full-scale production system.",
        "Solely engineered a critical internal security portal using Next.js to automate IT asset management and employee offboarding, addressing a major security vulnerability.",
        "Designed and implemented a system that integrated Azure AD Graph and on-premise MySQL data to create a unified, real-time dashboard of all company-issued devices for HR and IT admins.",
        "Developed a key automation that instantly revoked network access (via MAC address) for devices of offboarded or on-leave employees, completely eliminating the security risk of unauthorized access.",
        "Transformed the complex, multi-step manual employee offboarding process into a single-click action for HR, dramatically reducing administrative overhead and the potential for human error.",
        "The asset management portal is estimated to save the company ₹27 lakh (~$40,000) annually by preventing asset loss (~₹15 lakh) and reducing operational overhead (~₹12 lakh)."
      ]
    },
    {
      "job_title": "Software Development Engineer (Financial Systems)",
      "company": "Vivriti Capital",
      "location": "Chennai, KA, IN",
      "start_date": "2023-07",
      "end_date": "2023-08",
      "narrative_context": "Initial project assignment meeting: management team looking for project to assign, going through business problems solvable through tech. Manager, all interns, CTO, chairman there to introduce company and present problems. Mentioned all problems as formality, mentioned biggest problem: Credit Information Reports. I pushed them on it, they provided details, said waiting for multi-modal LLMs to solve issue. I said we could possibly tackle today. They laughed me off saying many tried, no one successful until now, just mentioned as formality. I said since no problem set for us, give our team chance to prove value. They said fine, next day we were working on it. Issue: Vivriti Capital is business lender, never asks for collateral, issues loans based on credit history and company history. Main document: company's credit information report ordered from third-party vendor who refused API access, simply placed PDF in S3 bucket when requested. Small businesses: 100 pages or less. Medium businesses: 1000 pages. Large enterprises: even more. Vivriti lended to all three. Depending on report size, had to source associates on contract. Assume 2 pages/day per associate, so 1000-page report = 35 associates for 15 days for quick decision, otherwise normal for each report to take 6+ months. Associates went through every page, populated into spreadsheet. Excel formula ran through spreadsheet, metrics to generate numbers they wanted. Finally, spreadsheet data aggregated from multiple spreadsheets from each associate, sent as final report to management who decided whether to give business money. Extremely inefficient process, entirely restricts business. To even think about giving company money is massive resource investment. Wouldn't it be nice if something could parse reports automatically, segregate sections, auto-populate into DBs, generate managerial report automatically, let associates pull data at button click for verification instead of going through 100 spreadsheets? And if system wouldn't make mistakes because humans prone to error? Technology team tried 100 different things before giving up. PDF unstructured, couldn't select text (disabled on PDF, vendor said to protect business model). Eventually gave up, AI team said when multi-modal AI comes out, will deal with problem. But I realized if this were my business I'm bleeding associates, salaries, decisions every day, so took initiative that our intern team could handle this monolithic problem. Took cues from previous attempts - their problem: sections didn't always start on new page, so OCR to extract text and regex to detect new sections didn't work well because had to iterate page by page, save current page text, check for new page. End up with blob of text didn't know how to process. I threw this model out. Why bother looking for all sections? Which specific sections most important? Which take most time to process? Selected those specific sections, hardcoded in code to look for these specific starting/ending section names. Realized only three sections longest and took most time. Hardcoded searched for these sections, processed separately. Key insight: old attempts tried to process every section the same. Every section was NOT the same - each section similar report to report, but section to section formats differed. Finally, structured section data into OpenSearch JSON object, sent to OpenSearch. Key: did not do population at this stage, just sent to OpenSearch. Reason: already provided massive value by letting employees automatically search for sections from portal instead of reading every page and writing to PDF. Worked surprisingly well because report to report same sections had same structuring. Targeted missile. Now that provided value, focus shifted to employee portal. How make data verifiable? Simple: along with text, note which page text came from. Give in employee portal option to check if text matched page. If yes, mark yes. If no, mark no and do required changes. Storing all this data. Most importantly, laying foundation for ML models to be fine-tuned on this rich correction data so in future can ensure high data integrity. Surprisingly data integrity already pretty good. Processed ~200 reports, 4 sections each, didn't have to make single edit because of robust extraction logic differing section to section. Architecture: PDF arrives in S3 bucket, S3 notif sends that new report arrived, report goes to back of queue, queue periodically checked (every 10 mins) to extract top PDF waiting to be processed, PDF forwarded to SpringBoot backend on EC2 creating required entries in OpenSearch, linking PDFs to right places. Used PDFPlumber because let us write custom PDF parsing code freely and easily. SpringBoot backend called our Python Lambda functions (4 functions per section) running in parallel. Made process faster, let sections be maintained separately so future dev teams can react to section changes quickly without affecting other sections. Once Lambda finished processing section, sent JSON to OpenSearch on IDs SpringBoot created. Completion sent to SpringBoot. After all four completion flags marked complete, PDF removed from queue. Once employees manually checked data on portal (step to ensure data integrity AND collect training data for ML models - two birds one stone, remove responsibility from my team because solution quickly built, not production ready). Once employees marked sections as secure, OpenSearch updated. Every time section confirmed, small hook ran to check if all four marked YES. After done, another Lambda called by SpringBoot microservice to generate managerial report. Delivered this solution in 2 months. Worked day and night, had to hand off tasks to other interns. Because manager didn't think we were doing anything useful, burden of proof fell on me. Had to talk to ML people, AI people - bled for this project. After 2 months told them internship was 6 months but proved this can be done in 2, please let me end early to focus on studies. They said they've seen what I can do, extended return offer (extended PPO for Tally too). Didn't join company, but one intern teammate joined on PPO. He said I laid foundation for whole process from thinking to give company money to deciding whether to give money from 3-6+ months to barely a day. New problem: management had so many managerial reports didn't know what to prioritize - good kind of problem. Did KT to other software engineers. Last heard they productionized my code fully, added logic for all other sections. After multi-modal AI came out, suddenly due to foundation I laid they had richest possible data available to train OCR or text extraction model from PDFs.",
      "key_metrics": {
        "report_size_small": "100 pages",
        "report_size_medium": "1000 pages",
        "associates_needed_for_1000_page": 35,
        "days_for_quick_decision": 15,
        "normal_processing_time": "6+ months",
        "pages_per_day_per_associate": 2,
        "sections_automated": 4,
        "reports_processed_in_pilot": 200,
        "manual_edits_needed": 0,
        "project_delivery_time": "2 months",
        "internship_duration": "6 months (ended early at 2 months)",
        "processing_time_after": "less than 1 day",
        "processing_time_before": "3-6+ months",
        "time_reduction": "from months to <1 day"
      },
      "role_clarity": "led (for PoC development), enabled (for production system and ML foundation)",
      "bullet_points": [
        "Initiated and led an engineering team to develop an automated data extraction pipeline for unstructured, 1000+ page PDF credit reports, reducing the loan analysis lifecycle from over 3 months to less than one day.",
        "Architected a scalable, event-driven system using AWS S3, Lambda, and a Spring Boot orchestrator to manage the ingestion and processing of financial documents, solving the company's primary operational bottleneck.",
        "Designed a novel parallel processing strategy, deploying distinct Python Lambda functions with custom PDFPlumber logic for each report section, enabling modularity and rapid, targeted data extraction.",
        "Structured and indexed the extracted financial data into OpenSearch, building a searchable repository that eliminated months of manual data entry for dozens of associates.",
        "Developed a verification portal that allowed employees to validate extracted data against the source PDF, ensuring 100% data integrity while simultaneously creating a high-quality, human-verified dataset for future ML model training.",
        "Delivered a fully functional prototype in 2 months, proving the problem was solvable with current technology despite skepticism from senior leadership, which led to the full productionization of the system.",
        "My pivotal role and the project's massive success in transforming the business's core workflow resulted in a Pre-Placement Offer (PPO) for a full-time role."
      ]
    }
  ],
  "projects": [
    {
      "project_name": "Jobbernaut",
      "technologies": [
        "Python",
        "OpenAI API",
        "Jinja2",
        "LaTeX",
        "Pydantic",
        "YAML",
        "CLI"
      ],
      "project_url": "https://jobbernaut.srmanda.com/",
      "narrative_context": "In job market, needed to track applications and what resume/cover letter used for each. Was using spreadsheet - inefficient, caused headache looking at rows. Designed simple tracker website to solve issue. Developed tracker in Lovable, posted on LinkedIn that made tracker to help people. JUST for that idea, 5 people messaged on LinkedIn wanting access code. Realized definite demand, did market research. Others selling similar spreadsheets for $5. Mine much simpler, free to use. But since made using Lovable, didn't trust it. Decided on new stack: Next.js for frontend, Postgres for DB, FastAPI for backend, Railway for deployment. While developing, realized spreadsheet although bad was doing job fine for me. What was my actual problem? Realized actual problem wasn't tracking, but tailoring application material to JD. That's why had tracking system - to know where sending which resume. Realized been trying to solve wrong problem. Had abstract problem now: automatically tailor every resume to JD. Difficult problem with very high standards. Realized main problems: hallucination, input only as good as output. Came up with idea of master_resume.json to ground product, came up with novel 12-step pipeline to take master resume and produce fully generated PDF and cover letter while ensuring accuracy EVERY time. Take master resume, choose right experiences and projects, rewrite using RSTA (STAR but results first), do company research along way, generate cover letter text using new tailored resume, generate LaTeX code with resume and cover letter text slotted in, have LLM compare both with original master resume to ensure grounded in reality with marketing leeway, run pdflatex to turn tex into PDF. Gave trial of command line tailoring tool to 5 close roommates. They said used it to apply to 50 jobs in a day, not single PDF or cover letter was fake, perfectly tailored to every JD. Realized monster I created, labeled Jobbernaut v1, took entire thing offline because believed had product that could disrupt market. Used YAML to store applications to make easy to make JD. Most importantly, uploaded one generated resume to Workday - DIDN'T HAVE TO CHANGE SINGLE THING. Everything auto-populated PERFECTLY without issue. Didn't touch single field. PDFs and cover letters no more than 30 KiB. Not just ATS compatible but ATS native - perfect parsing every time. Slowly iterated to v2 then v3. Now Jobbernaut Tailor extremely powerful tool don't intend to release to public, will secure IP very soon. Too powerful. Given JD fed to AI, Jobbernaut generated resumes ALWAYS graced MUST INTERVIEW category. Every application submitting now made fully by Jobbernaut. Now has dial to humanize output. Instead of asking AI to generate LaTeX code, now using Jinja templates, slowly iterating and productionizing. This very exercise solely to make sure Jobbernaut v3 has rich master resume to work with. Possibly cheapest AND most powerful application material generating tool on planet. Used to take 30-40 minutes to tailor application, now barely 30 seconds, integrity guaranteed and ensured to large extent. Out of 200 applications submitted all over world, not single time had to edit because LLM hallucinated.",
      "key_metrics": {
        "initial_interest": "5 LinkedIn messages for access",
        "competitor_price": "$5 for spreadsheets",
        "pipeline_steps": 12,
        "beta_testers": 5,
        "applications_by_testers": "50 jobs in a day",
        "workday_fields_edited": 0,
        "pdf_size": "<30 KiB",
        "versions_developed": 3,
        "tailoring_time_before": "30-40 minutes",
        "tailoring_time_after": "30 seconds",
        "total_applications_submitted": 200,
        "hallucination_incidents": 0,
        "ats_compatibility_score": "90%+ (Enhancv, Jobscan)"
      },
      "role_clarity": "led",
      "bullet_points": [
        "Built ATS-native resume tool ensuring candidate authenticity with zero noticed fabrications in 200+ applications",
        "Conceived and single-handedly developed Jobbernaut, an AI-powered CLI tool that automates the generation of tailored resumes and cover letters, reducing application creation time from over 30 minutes to just 30 seconds.",
        "Engineered a novel 12-step generation pipeline that grounds an LLM with a master resume JSON, intelligently selects relevant experiences, and rewrites them for a specific job description to eliminate hallucinations and ensure relevance.",
        "Implemented a system using Jinja and LaTeX to produce highly optimized (<30 KiB), ATS-native PDFs, achieving perfect, zero-correction parsing on major platforms like Workday.",
        "Developed a crucial verification step where an LLM cross-references the generated content against the source data to guarantee factual accuracy and content integrity before final output.",
        "The tool proved so effective in generating 'Must Interview' tier application materials that its public release was revoked due to ethical considerations and its potential to disrupt the job application market.",
        "The tool has been developed from the ground up with an anti-fragile, self-healing, and error-correcting architecture so that it learns from its mistakes and becomes better with every use.",
        "Enhancv, Jobscan and other ATS compatability tools give resumes generated with Jobbernaut no less than 90% compatibility score which is unheard of in the industry with a fully automated tool."
      ]
    },
    {
      "project_name": "MatchWise",
      "technologies": [
        "Python",
        "PuLP",
        "FastAPI",
        "Flutter",
        "Integer Linear Programming",
        "REST API"
      ],
      "project_url": "https://github.com/orgs/yashas-hm-unc/teams/523/repositories",
      "narrative_context": "Professors have to assign RAs to professors - PhD and MS students alike. Manually, this is 5-day process where 200 different factors need consideration: professor fit, availability, research interests, much other stuff. Teammates saw full stack application making process easier for professor. I saw Integer Linear Programming problem. Used PuLP. Objective function was custom probability function I designed generating best fit probability. Teammates worked on Flutter frontend while I worked on designing core ILP solution. Near project end, realized backend too tightly coupled with frontend, couldn't make backend changes without making frontend changes. Had to quickly pivot to designing blackbox functions in frontend and backend, decoupling architecture. Could both be hosted separately. Because optimized solving very well, response time rapid. Clicking match button returns fully matched set within 50-100ms tested using Postman. List of professors: [p1: [s1,s2,s3], p2: [s1,s2,s3]] where student order meant professor more interested in that student. Students had similar list. All data collected through forms before matching season starts. Research interests, professor research interest topics collected through CS internal directory, matching procedure done. About 30-40 professors and 60-100 students every year - made things lot easier. Didn't adopt it, but most importantly again proved it could be done. Communication through JSON payloads. Working with client on project, so genuine industrial grade software engineering project where had to understand client concerns, find pain points, choose right technologies (Flutter, FastAPI, PuLP), deliver software product in sprint with each person having different role. Very good project.",
      "key_metrics": {
        "manual_process_time": "5 days",
        "factors_considered": 200,
        "professors_count": "30-40",
        "students_count": "60-100",
        "response_time": "50-100ms",
        "team_size": "multiple members",
        "communication_method": "JSON payloads"
      },
      "role_clarity": "led (backend/ILP solution), contributed (overall project)",
      "bullet_points": [
        "Architected the core backend for MatchWise, an optimization tool that automates the assignment of ~100 graduate students to ~40 professors, reducing a 5-day manual process to under 200 milliseconds.",
        "Formulated the complex, multi-factor assignment challenge as an Integer Linear Programming (ILP) problem, implementing the solution with Python's PuLP library to find the optimal matches.",
        "Designed and implemented a custom objective function to maximize a 'best fit' probability score based on mutual professor-student rankings and shared research interests.",
        "Engineered a decoupled architecture with a FastAPI backend and a Flutter frontend, enabling independent development and ensuring rapid, scalable performance for the core matching algorithm.",
        "Successfully delivered the software as part of a client-facing project, demonstrating proficiency in translating ambiguous client requirements into a robust, high-performance technical solution."
      ]
    },
    {
      "project_name": "Cervical Cancer Detection",
      "technologies": [
        "Python",
        "DarkNet",
        "UMAP",
        "SVM",
        "Scikit-learn",
        "Image Processing",
        "ML"
      ],
      "project_url": "https://www.kaggle.com/code/ashishreddy9000/fyp-track-2-darknet-feat-ext-svm-quadri",
      "narrative_context": "Pipeline more complicated than typical. Used DarkNet to extract features. Used UMAP to reduce feature dimension to manageable state. Then used SVM to predict whether pap smear slide potential red flag or not, while leaning towards marking slide as red flag if model unsure. Why chose this specific pipeline out of thousand other options? Speed. Professor's mother was cancer survivor; she was hell bent on this cervical cancer problem. Multiple pap smear movements happened in India where hospitals do mass testing for cervical cancer, but processing single slide takes about 12 hours with so much manpower and technical expertise required. What if could make process just little bit faster? Like assistance to these doctors and medical specialists. That's what drove entire project. Didn't drive F1 score to 12% - that's fake. But idea tuned to real world, had lot of potential impact. Ensured through public data testing that given 10,000 pap smear slides (only like 100 or so but used image augmentation techniques to increase it) there will only be 1 or 2 slides missed. Had to leave project to go work in Tally, but yeah, this is what we did.",
      "key_metrics": {
        "manual_processing_time": "12 hours per slide",
        "test_dataset_size": "10,000 slides (augmented from ~100)",
        "miss_rate": "1-2 slides per 10,000",
        "miss_rate_percentage": "0.02%",
        "pipeline_stages": 3,
        "optimization_goal": "speed"
      },
      "role_clarity": "contributed",
      "bullet_points": [
        "Co-developed a multi-stage machine learning pipeline to accelerate the diagnosis of cervical cancer from pap smear slides, designed to reduce a 12-hour manual analysis to a rapid, computer-assisted process.",
        "Engineered a pipeline specifically optimized for speed and real-world clinical application, utilizing a pre-trained DarkNet model for feature extraction, UMAP for dimensionality reduction, and an SVM for final classification.",
        "The model was intentionally biased towards high recall (favoring false positives over false negatives) to minimize the risk of missed cases, ensuring its viability as a pre-screening tool for medical specialists.",
        "Validated the approach using image augmentation on public datasets, demonstrating a potential miss rate of less than 0.02% (1-2 slides per 10,000), proving the concept's value for large-scale screening programs."
      ]
    }
  ],
  "skills": {
    "Programming Languages": "Python, JavaScript, TypeScript, Java, SQL, KQL, C++, C#",
    "Frontend Technologies": "React, Next.js, Fluent UI, HTML/CSS, Flutter, Jinja2",
    "Backend & APIs": "Node.js, Flask, FastAPI, Spring Boot, REST API, GraphQL, Microservices",
    "Cloud & DevOps": "AWS Lambda, AWS S3, Azure AD, Kubernetes, Docker, Serverless, CI/CD",
    "Databases & Storage": "MySQL, PostgreSQL, Redis, OpenSearch, MongoDB, Database Design",
    "Data & ML": "Pandas, NumPy, Scikit-learn, PyTorch, NLP, ETL, Data Pipelines, UMAP",
    "Tools & Platforms": "Git, Jira, Trello, LaTeX, Linux, Agile, Microsoft Teams",
    "Specialized Skills": "Algorithm Design, System Architecture, Performance Optimization, Security"
  }
}
