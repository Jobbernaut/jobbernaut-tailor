{
  "contact_info": {
    "first_name": "Snehashish Reddy",
    "last_name": "Manda",
    "phone": "(919) 672-2226",
    "email": "srmanda.cs@gmail.com",
    "location": "Chapel Hill, NC",
    "linkedin_url": "https://www.linkedin.com/in/srmanda-cs",
    "github_url": "https://github.com/srmanda-cs",
    "portfolio_url": "https://srmanda.com"
  },
  "education": [
    {
      "institution": "UNC Chapel Hill",
      "degree": "Master of Science in Computer Science (GPA: 4.0/4.0)",
      "start_date": "2024-08",
      "graduation_date": "2026-05"
    },
    {
      "institution": "Amrita Vishwa Vidyapeetham",
      "degree": "Bachelor of Technology in Computer Science (GPA: 8.86/10.0)",
      "start_date": "2020-10",
      "graduation_date": "2024-06"
    }
  ],
  "work_experience": [
    {
      "job_title": "Software Development Engineer (Platform)",
      "company": "UNC Chapel Hill",
      "location": "Chapel Hill, NC",
      "start_date": "2025-08",
      "end_date": "Present",
      "narrative_context": {
        "technical_story": "Built Python autograding platform using Pandas, NumPy, and Sentence-Transformers NLP. Core challenge: matching 240 student names from PollEv (inconsistent formatting, nicknames, typos) to Canvas gradebook. Engineered multi-stage algorithm: exact match, fuzzy matching, semantic similarity via Sentence-Transformers embeddings, statistical name analysis. Designed atomic grade-update ensuring data integrity - only modifies empty fields (dashes), never overwrites existing grades. Implemented participation threshold: students answering >50% of polls auto-credited. Built version control for scripts enabling rollback. Automated gradebook backups. Script downloads Canvas gradebook, accesses PollEv via spreadsheet mapping, merges multiple spreadsheets from different class times.",
        "leadership_story": "Guided 2 Undergraduate Learning Assistants on research and logistics while bringing CS automation into Neuroscience course. Championed Jira and Trello adoption for 10-person team by running pilot with one grader while others observed. When work completed faster, showed examples from experienced professors and education templates. Eventually convinced entire team despite initial resistance. Collaborated with professor and 4 GTAs as peer contributor. When tool succeeded, instructor and TA from another course approached to adopt. Prepared Memory Systems II lecture 3 months in advance. Received $5,000 award after reaching out about tuition costs.",
        "impact_story": "Reduced team workload 60% from 20 hours required to 8 hours actual work weekly. Autograder eliminated 36 minutes daily manual work per GTA (6 PollEvs × 60 students × 10 seconds = 360 updates). Process improvements (Jira, Trello, rubrics) saved 8-9 hours weekly per person. Standardized rubrics reduced grading from 5-6 minutes to 30 seconds per assignment. Tool adopted by 13 instructors across 2 courses. Positioned for university-wide AWS Lambda deployment.",
        "challenges_overcome": "Initial challenge: 360 grade updates per GTA per class at 10 seconds each = 36 minutes daily of manual entry. Faced skepticism about automation. Built working prototype to prove value. Second challenge: PollEv names inconsistent vs Canvas roster. Solved with NLP semantic matching plus statistical analysis. Third challenge: convincing team to adopt Jira/Trello. Ran pilot, demonstrated results, showed professor templates. Fourth challenge: data integrity. Solved with atomic updates and automated backups. Fifth challenge: bot detection. Solved with rate limiting and proper authentication.",
        "technologies_deep_dive": "Python for automation. Pandas for DataFrame operations and CSV manipulation. NumPy for statistical analysis. Sentence-Transformers for semantic name matching using embeddings. Canvas API for gradebook downloads. PollEv spreadsheet integration. Git for version control. Jira and Trello for project management. Designed for AWS Lambda deployment for university scaling. Atomic transaction design - only pulls PollEv view, if number exists (0/1) cannot modify, only dashes change."
      },
      "key_metrics": {
        "students_managed": 240,
        "course_sections": 2,
        "gtas_on_team": 4,
        "grade_updates_per_person_per_class": 360,
        "manual_time_per_person_per_day": "36 minutes",
        "manual_time_per_person_per_week": "180 minutes",
        "pollev_questions_per_class": 6,
        "total_instructors_using_tool": 13,
        "courses_adopted": 2,
        "grading_time_before": "5-6 minutes per assignment",
        "grading_time_after": "30 seconds per assignment",
        "weekly_hours_required_by_policy": 20,
        "weekly_hours_after_optimization": 8,
        "workload_reduction_percentage": "60%",
        "award_amount": "$5,000"
      },
      "role_clarity": "contributed"
    },
    {
      "job_title": "Software Development Engineer (Internal Systems)",
      "company": "Tally",
      "location": "Bengaluru, KA, IN",
      "start_date": "2024-01",
      "end_date": "2024-05",
      "narrative_context": {
        "technical_story": "PMS PoC: Architected Microsoft Teams app using ReactJS, Fluent UI, Node.js. Chose Teams because runs React websites, Microsoft-backed, phone-accessible. Backend: Flask on MySQL (matching legacy PMS) for easy migration. Designed transparent goal hierarchy: daily, weekly, monthly, quarterly, half-yearly, annual. Employees set personal goals, management sets organizational goals trickling down. Generated leaderboards and management reports showing completion rates. Asset Portal: Built Next.js app integrating Azure AD Graph with on-premise MySQL device data. Designed algorithms alerting HR/IT on inactive devices. Extended scope: automatic device disabling when employee on leave or removed from payroll via MAC address. Scripts aggregate data, confirm employee ID, revoke network access.",
        "leadership_story": "Led 3-person intern team under manager. Manager delegated intern coordination to me due to strong communication and technical knowledge. Mapped code/architectural needs, split work, drove development. Interviewed everyone in performance review process to map architecture. Manager championed new PMS, put me in charge of proving viability to skeptical management. Pressure: hadn't finished college, managing 2 interns, 3-month deadline, management opposition. After PoC success, manager got promotion. I was reassigned to asset portal while other interns struggled for projects.",
        "impact_story": "PMS PoC proved viability, resulting in ₹7 crore (~$1M) budget approval for production system. Handed to senior engineering team. Asset Portal: estimated ₹27 lakh (~$40K) annual savings - ₹15 lakh preventing asset loss, ₹12 lakh reducing overhead (2 roles redundant: 1 HR, 1 IT admin at ₹6 lakh each). Eliminated security risk of old employee devices accessing confidential info. Previous incident: old device leaked new version pricing to press, giving competitors advantage. Portal made impossible - employee on leave, device access immediately revoked. Past year: company lost ₹15 lakh in assets because no retrieval system after employees left.",
        "challenges_overcome": "PMS: 24-year legacy system nobody understood. Employees frustrated with lack of transparency. Management skeptical because current system 'worked fine'. Biggest issue: no feedback mechanism. When SLM declined report, employee only saw rejection, no reason. 30-40 employees per SLM, no time for meetings. Everything approved to avoid headache. When properly declined, employees quit. Company churning talent. Had to build transparent pipeline, easy migration, feedback mechanism, good UX. Asset Portal: device deletion manual. Company lost ₹15 lakh in assets yearly. Convinced stakeholders automatic disabling acceptable security tradeoff despite inconvenience.",
        "technologies_deep_dive": "PMS: ReactJS for UI, Fluent UI for Microsoft design system, Node.js for app server, Flask for REST API, MySQL for database (legacy compatibility), Microsoft Teams platform. Asset Portal: Next.js for full-stack React, Azure AD Graph API for identity data, on-premise MySQL for device tracking, MAC address-based network access control. Integration layer merging Azure cloud with on-premise systems. Automated workflows triggered by HR actions cascading device revocations."
      },
      "key_metrics": {
        "team_size": 3,
        "interns_managed": 2,
        "legacy_system_age": "24 years",
        "pms_budget_approved": "₹7 crore (~$1M USD)",
        "project_delivery_time": "3 months",
        "assets_lost_per_year": "₹15 lakh (~$20,000 USD)",
        "estimated_annual_savings": "₹27 lakh (~$40,000 USD)",
        "roles_made_redundant": 2,
        "employees_under_slm": "30-40"
      },
      "role_clarity": "led (for PoC and asset portal), enabled (for final PMS investment)"
    },
    {
      "job_title": "Software Development Engineer (Financial Systems)",
      "company": "Vivriti Capital",
      "location": "Chennai, KA, IN",
      "start_date": "2023-07",
      "end_date": "2023-08",
      "narrative_context": {
        "technical_story": "Architected event-driven system: PDF arrives in S3, S3 notification triggers queue, queue checked every 10 minutes, top PDF forwarded to Spring Boot backend on EC2. Spring Boot creates OpenSearch entries, calls 4 parallel Python Lambda functions (one per section) using PDFPlumber for custom parsing. Key decision: didn't process all sections uniformly. Old attempts failed treating sections identically. Each section similar report-to-report but differed section-to-section. Hardcoded search for 3 specific longest sections, processed separately with section-specific logic. Structured data into OpenSearch JSON. Didn't populate initially - just sent to OpenSearch, providing value by letting employees search sections instead of reading every page. Built verification portal: employees check extracted text against source page, mark yes/no, make corrections. Stores correction data as ML training foundation.",
        "leadership_story": "At assignment meeting, management mentioned Credit Information Reports as formality, waiting for multi-modal LLMs. I pushed for details, said we could tackle today. They laughed saying many tried, none successful. I insisted our team get chance. Next day working on it. Worked day and night, delegated to other interns. Burden of proof fell on me because manager skeptical. Talked to ML people, AI people. After 2 months, told them proved this in 2 months of 6-month internship, requested early end for studies. Extended return offer (also from Tally). Didn't join, but teammate joined on PPO. Did knowledge transfer to engineers who productionized code.",
        "impact_story": "Reduced loan analysis from 3-6+ months to <1 day. Previously: 1000-page report required 35 associates for 15 days (at 2 pages/day) for quick decision, otherwise 6+ months. Associates manually populated spreadsheets, Excel generated metrics, data aggregated, sent to management. Massive resource investment just to consider lending. My solution: automated extraction, searchable OpenSearch repository, verification portal. Processed ~200 reports, 4 sections each, zero manual edits needed. Teammate said I transformed process from thinking about lending to deciding in <1 day. New problem: management had too many reports to prioritize (good problem). Foundation enabled richest training data when multi-modal AI arrived.",
        "challenges_overcome": "Initial skepticism: tech team tried 100 approaches before giving up. AI team said wait for multi-modal AI. Management mentioned as formality. I convinced them to try. Technical: PDF text selection disabled by vendor. Previous attempts used OCR + regex page-by-page, ended with unusable text blob. My insight: threw out that model. Why process all sections? Identified 3 most important, time-consuming sections. Hardcoded search for specific section names. Targeted approach. Each section got custom parsing because formats differed. Data integrity: verification portal served dual purpose - ensure accuracy AND collect ML training data. Removed production-readiness responsibility from team because PoC quickly built.",
        "technologies_deep_dive": "AWS S3 for storage and event triggers. AWS Lambda for serverless compute - 4 Python functions per section, parallel processing. PDFPlumber for custom PDF parsing with section-specific logic. Spring Boot on EC2 for orchestration, queue management, OpenSearch coordination. OpenSearch for structured data storage and search. Queue-based processing (10-minute intervals) for reliability. Event-driven architecture: S3 notification, queue, Spring Boot, parallel Lambdas, OpenSearch, verification portal, final report generation. Parallel design enabled future teams to modify sections independently."
      },
      "key_metrics": {
        "report_size_medium": "1000 pages",
        "associates_needed_for_1000_page": 35,
        "days_for_quick_decision": 15,
        "normal_processing_time": "6+ months",
        "pages_per_day_per_associate": 2,
        "sections_automated": 4,
        "reports_processed_in_pilot": 200,
        "manual_edits_needed": 0,
        "project_delivery_time": "2 months",
        "processing_time_after": "less than 1 day",
        "processing_time_before": "3-6+ months"
      },
      "role_clarity": "led (for PoC development), enabled (for production system and ML foundation)"
    }
  ],
  "projects": [
    {
      "project_name": "Jobbernaut Tailor",
      "technologies": [
        "Python",
        "FastAPI",
        "OpenAI API",
        "Jinja2",
        "LaTeX",
        "Pydantic",
        "YAML",
        "CLI"
      ],
      "project_url": "https://github.com/Jobbernaut/jobbernaut-tailor",
      "narrative_context": {
        "technical_story": "Engineered 13-step intelligence-driven pipeline with 3-stage intelligence gathering phase executing BEFORE content generation: (1) Job resonance analysis extracting emotional keywords, cultural values, hidden requirements, power verbs, and technical keywords, (2) Company research mapping mission, values, tech stack, and culture, (3) Storytelling arc generation creating narrative hooks and proof points. Pipeline then: (4) Validates job inputs, (5) Generates tailored resume using intelligence context, (6) Validates with Pydantic, (7) Applies humanization, (8) Renders LaTeX via Jinja2 templates, (9-13) Compiles PDFs and organizes output. Built master_resume.json as single source of truth grounding all LLM outputs. Implemented multi-layer validation: input validation → Pydantic schema validation → quality threshold validation (character counts, array sizes, meaningful content). Designed generic retry wrapper with progressive error feedback enabling self-healing across all intelligence steps. Fully open source with comprehensive technical documentation (7 guides: ARCHITECTURE, EVOLUTION, PIPELINE, MODELS, CONFIGURATION, TEMPLATES, ANTI_FRAGILE_PIPELINE). PDFs optimized to <30 KiB. Workday: uploaded resume, zero fields needed editing, perfect auto-population.",
        "leadership_story": "Conceived and single-handedly developed entire system from tracker idea to production CLI tool. Started with Lovable tracker, posted on LinkedIn, received 5 access requests. Did market research, found competitors selling spreadsheets for $5. Realized actual problem wasn't tracking but tailoring. Pivoted completely. Gave trial to 5 roommates as beta testers. They applied to 50 jobs in a day with zero fake content, perfectly tailored. Labeled Jobbernaut v1, took offline because realized market disruption potential. Iterated through v2 (production freeze with LaTeX verification), v3 (template revolution with Jinja2), and v4 (intelligence-driven architecture). Open sourced entire codebase with professional documentation enabling community adoption. Architected system to be anti-fragile: learns from mistakes, self-healing via progressive error feedback, error-correcting with quality validation.",
        "impact_story": "Reduced tailoring from 30-40 minutes to 30 seconds (98% reduction). Beta testers applied to 50 jobs in single day maintaining quality. Out of 200+ applications worldwide, zero instances of editing needed due to hallucination. Achieved 90%+ ATS compatibility on Enhancv and Jobscan (unheard of for automated tool). Generated resumes consistently graced 'must interview' category. Perfect Workday parsing with zero manual corrections. PDFs <30 KiB ensuring fast processing. Open sourced with 7 comprehensive technical documentation guides enabling developers to understand, extend, and contribute. Intelligence-driven architecture produces more targeted, compelling applications through contextual understanding before generation. Proof of effectiveness: the resume you're reading was generated by Jobbernaut.",
        "challenges_overcome": "Initial: preventing LLM hallucinations. Solved by grounding with master_resume.json and multi-layer validation (input → Pydantic → quality thresholds). Challenge: ATS compatibility. Solved using LaTeX for precise formatting control and PDF size optimization. Moved from LLM-generated LaTeX to Jinja templates for reliability. Challenge: factual accuracy. Solved with Pydantic schema validation, quality threshold validation, and explicit verification prompts. Challenge: balancing automation speed with quality. Solved with 13-step pipeline where each step has single responsibility. V4 challenge: reactive content generation producing generic outputs. Solved by separating intelligence gathering from content generation - understand job context first, then generate targeted content. Challenge: ensuring meaningful content beyond structural validity. Solved with quality validation layer checking character counts, array sizes, and content substance. Challenge: code reusability across intelligence steps. Solved with generic retry wrapper handling all intelligence steps uniformly with progressive error feedback.",
        "technologies_deep_dive": "Python for pipeline orchestration. OpenAI API (Poe AI) for intelligent content generation across multiple models (Gemini, Claude, GPT-4). Pydantic for strict JSON schema validation preventing malformed outputs. Quality validation layer beyond Pydantic ensuring meaningful content (character thresholds, array size constraints, empty string detection). Jinja2 templating with custom delimiters (\\VAR{}, \\BLOCK{}) for LaTeX code generation (replaced direct LLM generation). LaTeX for precise PDF formatting and ATS compatibility. YAML for human-readable application storage. CLI built with Python argparse. pdflatex compiler for PDF generation. Git for version control. Generic retry wrapper with progressive error feedback enabling self-healing. Configurable AI models per intelligence step optimizing for speed, creativity, or accuracy. Fail-fast input validation preventing wasted API calls. Designed as anti-fragile system: learns from mistakes, self-healing, error-correcting. Each pipeline step isolated for independent testing. Comprehensive documentation system with 7 technical guides."
      },
      "key_metrics": {
        "initial_interest": "5 LinkedIn messages",
        "pipeline_steps": 13,
        "intelligence_steps": 3,
        "beta_testers": 5,
        "applications_by_testers": "50 jobs in a day",
        "workday_fields_edited": 0,
        "pdf_size": "<30 KiB",
        "versions_developed": 4,
        "documentation_pages": 7,
        "tailoring_time_before": "30-40 minutes",
        "tailoring_time_after": "30 seconds",
        "total_applications_submitted": 200,
        "hallucination_incidents": 0,
        "ats_compatibility_score": "90%+",
        "open_source": true
      },
      "role_clarity": "led"
    },
    {
      "project_name": "MatchWise",
      "technologies": [
        "Python",
        "PuLP",
        "FastAPI",
        "Flutter",
        "Integer Linear Programming",
        "REST API"
      ],
      "project_url": "https://github.com/orgs/yashas-hm-unc/teams/523/repositories",
      "narrative_context": {
        "technical_story": "Formulated graduate student-to-professor assignment as Integer Linear Programming problem using Python's PuLP library. Designed custom objective function generating 'best fit' probability score based on mutual rankings and research interest alignment. Input: professors list with ranked student preferences, students list with ranked professor preferences, research interests from CS directory. Constraint: each student assigned to exactly one professor, professor capacity limits respected. Optimization goal: maximize total fit probability. Built FastAPI backend exposing matching algorithm as REST API. Teammates developed Flutter frontend. Near project end, realized tight coupling. Quickly pivoted to decoupled architecture with blackbox functions, enabling independent hosting.",
        "leadership_story": "Led backend development and core ILP solution design while teammates worked on Flutter frontend. Worked with client (CS department) to understand pain points in manual matching. Identified 5-day process considering 200+ factors. Recognized as optimization problem rather than just UI problem. Delivered software in sprint-based approach with each team member having distinct role. Communicated via JSON payloads. Genuine industrial-grade software engineering project requiring client requirement gathering, technology selection justification, product delivery.",
        "impact_story": "Reduced matching from 5 days manual work to 50-100ms automated response (99.9% reduction). Handles 30-40 professors and 60-100 students annually. Considers 200 different factors automatically (professor fit, availability, research interests, mutual preferences). Eliminates human bias, ensures optimal global matching rather than greedy local decisions. Clicking match button returns fully matched set within 100ms tested via Postman. While not adopted by department, proved concept viable and demonstrated optimization approach to complex assignment problem.",
        "challenges_overcome": "Challenge: formulating fuzzy matching problem as mathematical optimization. Solved by designing probability-based objective function quantifying 'fit' from qualitative preferences. Challenge: handling conflicting preferences (student wants professor A, but professor A prefers different student). Solved by ILP constraint satisfaction finding globally optimal solution. Challenge: tight coupling between frontend and backend limiting independent development. Solved by refactoring to blackbox API functions with clear JSON contracts. Challenge: ensuring fast response time for real-time matching. Solved by optimizing PuLP solver configuration and constraint formulation.",
        "technologies_deep_dive": "Python for backend logic and optimization. PuLP (Python Linear Programming) library for ILP problem formulation and solving. FastAPI for high-performance REST API with automatic OpenAPI documentation. Flutter for cross-platform mobile/web frontend. JSON for API communication contracts. Integer Linear Programming for constraint satisfaction and optimization. REST API architecture for decoupled frontend-backend communication. Postman for API testing and performance validation. Research interest matching using text similarity algorithms."
      },
      "key_metrics": {
        "manual_process_time": "5 days",
        "factors_considered": 200,
        "professors_count": "30-40",
        "students_count": "60-100",
        "response_time": "50-100ms",
        "time_reduction_percentage": "99.9%"
      },
      "role_clarity": "led (backend/ILP solution), contributed (overall project)"
    },
    {
      "project_name": "Cervical Cancer Detection",
      "technologies": [
        "Python",
        "DarkNet",
        "UMAP",
        "SVM",
        "Scikit-learn",
        "Image Processing",
        "ML"
      ],
      "project_url": "https://www.kaggle.com/code/ashishreddy9000/fyp-track-2-darknet-feat-ext-svm-quadri",
      "narrative_context": {
        "technical_story": "Built 3-stage ML pipeline: (1) DarkNet pre-trained model for feature extraction from pap smear slide images, (2) UMAP for dimensionality reduction making features manageable for classification, (3) SVM for binary classification (red flag vs normal). Intentionally biased SVM towards high recall (favoring false positives over false negatives) to minimize missed cancer cases. Used image augmentation techniques expanding ~100 real slides to 10,000 training samples. Validated on augmented test set achieving 0.02% miss rate (1-2 slides per 10,000). Pipeline specifically optimized for speed to assist doctors in mass screening programs.",
        "leadership_story": "Contributed to team project driven by professor whose mother was cancer survivor. Professor hell-bent on cervical cancer problem. Collaborated on designing pipeline optimized for real-world clinical application rather than just F1 score. Had to leave project early to work at Tally, but established foundation for speed-optimized diagnostic assistance tool. Worked with team to ensure model tuned for practical impact in mass testing scenarios where hospitals process slides requiring 12 hours manual analysis each.",
        "impact_story": "Designed to reduce 12-hour manual slide analysis to rapid computer-assisted pre-screening. In mass testing movements in India, hospitals do cervical cancer screening but processing single slide takes 12 hours with significant manpower and expertise. Pipeline aimed to make process faster as assistance to doctors and medical specialists. Achieved 0.02% miss rate on 10,000-slide test set, prioritizing patient safety over raw accuracy. Model intentionally biased towards high recall to minimize critical false negatives in diagnostics.",
        "challenges_overcome": "Challenge: balancing speed with accuracy for clinical viability. Solved by choosing DarkNet for fast feature extraction, UMAP for efficient dimensionality reduction, SVM for quick classification. Challenge: ensuring patient safety. Solved by intentionally biasing model towards high recall, accepting more false positives to minimize dangerous false negatives. Challenge: limited real training data (~100 slides). Solved with image augmentation techniques expanding dataset to 10,000 samples. Challenge: real-world applicability. Focused on speed optimization and practical deployment rather than just maximizing F1 score.",
        "technologies_deep_dive": "Python for pipeline implementation. DarkNet pre-trained deep learning model for robust feature extraction from medical images. UMAP (Uniform Manifold Approximation and Projection) for dimensionality reduction preserving local structure. SVM (Support Vector Machine) from Scikit-learn for binary classification with adjustable decision boundary. Image augmentation techniques (rotation, flipping, scaling) for dataset expansion. Scikit-learn for ML utilities and model evaluation. Pipeline designed for speed: feature extraction, dimensionality reduction, classification optimized for rapid processing to assist doctors in mass screening scenarios."
      },
      "key_metrics": {
        "manual_processing_time": "12 hours per slide",
        "test_dataset_size": "10,000 slides (augmented from ~100)",
        "miss_rate": "1-2 slides per 10,000",
        "miss_rate_percentage": "0.02%",
        "pipeline_stages": 3,
        "optimization_goal": "speed and patient safety"
      },
      "role_clarity": "contributed"
    }
  ],
  "skills": {
    "Programming Languages": "Python, JavaScript, TypeScript, Java, SQL, KQL, C++, C#",
    "Frontend Technologies": "React, Next.js, Fluent UI, HTML/CSS, Flutter, Jinja2",
    "Backend & APIs": "Node.js, Flask, FastAPI, Spring Boot, REST API, GraphQL, Microservices",
    "Cloud & DevOps": "AWS Lambda, AWS S3, Azure AD, Kubernetes, Docker, Serverless, CI/CD",
    "Databases & Storage": "MySQL, PostgreSQL, Redis, OpenSearch, MongoDB, Database Design",
    "Data & ML": "Pandas, NumPy, Scikit-learn, PyTorch, NLP, ETL, Data Pipelines, UMAP",
    "Tools & Platforms": "Git, Jira, Trello, LaTeX, Linux, Agile, Microsoft Teams",
    "Specialized Skills": "Algorithm Design, System Architecture, Performance Optimization, Security"
  }
}
